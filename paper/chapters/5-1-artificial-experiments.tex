\subsubsection{Case 1}
\label{sec:case-1}

In this example, we evaluate the efficiency of the two approximations, \(f_{\mathtt{DALE}}\) and \(\hat{f}_{\mathtt{ALE}}\), through the execution times. We want to compare how both approximations perform in terms of the dimensionality of the problem (\(D\)), the dataset size (\(N\)) and the size of the model \(f\).

In each experiment we generate a dataset \( X \), by drawing \( N \cdot D \) samples from a standard normal distribution. The black-box function \(f \) is a fully-connected neural network with \(L\) hidden layers of \( 1024 \) units each. All experiments are done using \(K=100\). We want to clarify that the value of \(K\) plays almost no role in the execution times. Since we are interested in solely comparing the methods efficiency, we do not provide further details for the modelling part.

In Figure~\Ref{fig:case-1-plots-1}, we directly compare \(f_{\mathtt{DALE}}\) and \(\hat{f}_{\mathtt{ALE}}\) in two different setups: in Figure~\Ref{fig:case-1-plots-1}(Left), we use a light setup of \(N=10^3\) examples and a model of \(L=2\) layers, whereas in Figure~\Ref{fig:case-1-plots-1}(Right), a heavier setup with \(N=10^5\) and \(L=6\). We observe that in both cases, DALE executes in near-constant time independently of \(D\), while ALE scales linearly with wrt \(D\), confirming our claims of Section~\ref{sec:4-2-computational}. The difference in the execution time reaches significant levels for relatively small dimensionalities. In the heavy setup, ALE needs almost a minute for \(D=20\), three minutes for \(D=50\), and 15 minutes for \(D=100\). In all these cases, DALE executes in a few seconds.

Another critical remark is that DALE's execution time is almost identical to the computation of the Jacobian \( \Jac \), which is benefited by automatic differentiation. Hence, we confirm that the overhead of performing steps 3-5 of Algorithm~\ref{alg:dale} is a small fraction of the total execution time. Another consequence of this remark is that we can test many different bin sizes with near-zero computational cost.

In Figure~\Ref{fig:case-1-plots-2}, we rigorously quantify to what extent the dataset size \(N\) and the model size \(L\) affect both methods. In Figures~\Ref{fig:case-1-plots-2}(a) and ~\Ref{fig:case-1-plots-2}(b), we confirm that both \(N\) and \(L\) impact how ALE complexity increases wrt \(D\). Therefore, for a big dataset and a heavy model \(f\), ALE's execution time quickly reaches prohibitive levels. In contrast, in Figures~\Ref{fig:case-1-plots-2}(c) and Figures~\Ref{fig:case-1-plots-2}(d), DALE is negligibly affected by these parameters. In the figures, we restrict the experiment to cases up to 100-dimensional input for illustration purposes. The same trend continues for an arbitrary number of dimensions. DALE can scale efficiently to an arbitrary number of dimensions as long as we have enough resources to store the dataset, evaluate the prediction model \(f\) and apply the gradients \(\nabla_{\xb}f\).

\begin{figure}[h]
  \centering
  \resizebox{.4\columnwidth}{!}{\input{./images/case-1-plot-1.tex}}
  \resizebox{.43\columnwidth}{!}{\input{./images/case-1-plot-2.tex}}
  \caption[Example 3]{Comparison of the execution time of DALE
    and ALE in two setups: (Left) Light setup; \(N=10^3, L=2\).
    (Right) Light setup; \(N=10^5, L=6\)}
  \label{fig:case-1-plots-1}
\end{figure}

\begin{figure}[h]
  \centering
  \resizebox{.23\columnwidth}{!}{\input{./images/case-1-plot-3.tex}}
  \resizebox{.23\columnwidth}{!}{\input{./images/case-1-plot-4.tex}}
  \resizebox{.23\columnwidth}{!}{\input{./images/case-1-plot-5.tex}}
  \resizebox{.23\columnwidth}{!}{\input{./images/case-1-plot-6.tex}}
  \caption[Example 3]{Measurements of the execution time wrt dimensionality \(D\):
    (a) \(\hat{f}_{\mathtt{ALE}}\) for \(L = 2\), and many dataset sizes \(N\)
    (b) \(\hat{f}_{\mathtt{ALE}}\) for \(N = 10^3\), and many model sizes \(L\)
    (c) \(f_{\mathtt{DALE}}\) for \(L = 2\), and many dataset sizes \(N\)
    (d) \(f_{\mathtt{DALE}}\) for \(N = 10^3\), and many model sizes \(L\)
  }
  \label{fig:case-1-plots-2}
\end{figure}


\subsubsection{Case 2}
\label{sec:example2}

In this example, we evaluate the accuracy of the two approximations,
\(f_{\mathtt{DALE}}\) and \(\hat{f}_{\mathtt{ALE}}\), in a synthetic
dataset where the ground truth ALE is accesible. As discussed in
Section~\ref{sec:4-3-robustness}, ALE approximation is vulnerable to
OOD sampling when the bins are wide, or equivalently, the number of
bins \(K\) is small. We want to compare how both approximations behave
in a case where the local effect is noisy.

We design an experiment where we know the black-box function and the
data generating distribution. The black-box function
\(f:\R^3 \rightarrow \R\) is split in three parts to amplify the
effect of OOD sampling. It includes a mild term
\( f_0(x) = x_1x_2 + x_1x_3 \) in the region
\( 0 \leq |x_1 - x_2| < \tau \) and then a quadratic term
\(g(x) = \alpha ((x_1 - x_2)^2 - \tau^2)\) is added(subtracted)
over(under) the region, i.e.:

\begin{equation} \label{eq:example-2-mapping}
  f(\mathbf{x}) =
  \begin{cases}
    f_0(x) & , 0 \leq |x_1 - x_2|  < \tau \\
    f_0(x) - g(x) & , \tau \leq |x_1 - x_2|  \\
    f_0(x) + g(x) & , \tau \leq - |x_1 - x_2|  \\
  \end{cases}
\end{equation}

\noindent

The data points \(X^i = (x_1^i, x_2^i, x_3^i)\) are generated as
follows; \(x_1^i \) are clustered around the points
\(\{1.5, 3, 5, 7, 8.5\}\),
\(x_2^i \sim \mathcal{N}(\mu=x_1, \sigma_2=0.1) \) and
\(x_3^i \sim \mathcal{N}(\mu=0, \sigma_3^2=10) \). In
Figure~\ref{fig:example-2-samples}(a), we illustrate \(f(\xb)\) for
\(x_3=0\), as well as the generated data points.

In this example, the local effect of \(x_1\) is
\(\frac{\partial f}{\partial x_1} = x_2 + x_3\). Due to the noisy
nature of \(x_3\), both ALE and DALE need a large number of sample for
robust estimation. Therefore, we need to lower the number of bins
\(K\). As will be shown below, both ALE and DALE fail to approximate
the feature effect for high \(K\). On the other hand, when using a
lower \(K\), ALE approximation fails due to OOD sampling, while DALE
manages to accurately approximate the feature effect.

In Figure~\ref{fig:example-2-samples}(b) and
Figure~\ref{fig:example-2-samples}(c), we observe the estimated
effects for \(K=50\) and \(K=5\). In
Figure~\ref{fig:example-2-samples}(b), \((K=50)\) the approximations
converge to the same estimated effect which is inaccurate due to many
noisy artifacts. In Figure~\ref{fig:example-2-samples}(c), \((K=5)\)
we observe that for small \(K\), DALE approximates the ground-truth
effect well, whereas ALE fails due to OOD
sampling. Table~\ref{tab:case-2-accuracy} provides the NMSE of both
approximation for varying number of bins \(K\). We observe that DALE
consistently provides accurate estimations (NMSE \(\leq 0.1\)) for all
small \(K\) values.

The experiments helps us confirm that when \(K \) increases, both
approximations are based on a limited number of samples, and are
vulnerable to noise. When \(K\) decreases, DALE lowers the resolution
but provides more robust estimations. In contrast, ALE is vulnerable
to OOD sampling.

\begin{table}
  \centering
  \caption{Evaluation of the NMSE between the approximations and the ground truth. Blue color indicates the values that are below \(0.1\).}
  \label{tab:case-2-accuracy}
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \multicolumn{10}{c}{Accuracy on the Synthetic Dataset (Case 2)} \\
    \hline \hline
    & & \multicolumn{8}{|c}{Number of bins} \\
    \hline
    & & 1 & 2 & 3 & 4 & 5 & 10 & 20 & 40 \\
    \hline
    \hline
    \multirow{2}{*}{\(\mathtt{NMSE}\)} & \(\dale\) & \textcolor{blue}{0.10} & \textcolor{blue}{0.03} & \textcolor{blue}{0.09} & \textcolor{blue}{0.02} & \textcolor{blue}{0.02} & 0.82 & 0.24 & 0.38\\
    & \(\alep\) & 100.42 & 22.09 & 4.97 & 2.81 & 0.78 & 1.49 & 0.34 & 0.39 \\
    \hline
  \end{tabular}
\end{table}


\begin{figure}[h]
  \begin{center}
    \resizebox{.33\columnwidth}{!}{\input{./images/case-2-f-gt.tex}}
    \resizebox{.32\columnwidth}{!}{\input{./images/case-2-fe-bins-50.tex}}
    \resizebox{.32\columnwidth}{!}{\input{./images/case-2-fe-bins-5.tex}}
  \end{center}
  \caption[Example 2]{Example 2: Samples and (a) actual black box function and
    (b) black box function estimated via a neural network}
  \label{fig:example-2-samples}
\end{figure}

