Explainable AI (XAI) is a fast-evolving field with a growing
interest. In recent years, the domain has matured by establishing its
terminology and objectives~\cite{Hoffman2018}. Several surveys have
been published~\cite{BarredoArrieta2020}, % ~\cite{Adadi2018}
classifying
the different approaches and detecting future challenges on the
field~\cite{Molnar2020}.

There are several criterias for grouping XAI methods. A very popular
distinction is between local and global ones. Local interpretability
methods explain why a model made a specific prediction given a
specific input. For example, local surrogates such as
LIME~\cite{Ribeiro2016} train an explainable-by-design model in data
points generated from a local area around the input under
examination. SHAP values~\cite{Lundberg2017} measures the contribution
of each attribute in a specific prediction, formulating a
game-theoretical framework based on Shapley
Values. Counterfactuals~\cite{Wachter2017} search for a data point as
close as possible to the examined input that flips the
prediction. Anchors~\cite{Ribeiro2018} provide a rule, i.e. a set of
attribute values, that is enough to freeze the prediction,
independently of the value of the rest of the attributes.


Global methods, which is the focus of this paper, explain the average
model behaviour. For example, prototypes~\cite{Gurumoorthy2019} search
for a data point that is a characteristic representative of a specific
class. Criticisms~\cite{Kim2016}, search for data points whose class
is ambiguous. Global feature importance methods characterize each
input feature by assigning to it an importance score. Permutation
feature importance~\cite{Fisher2019} measures the change in the
prediction score of a model, after permuting the value of each
feature. Often, apart from knowing that a feature is important, it
also valuable to know the type of the effect on the output
(positive/negative). Feature effect methods take a step further and
quantify the type of a each feature attribute influences the output on
average. There are three popular feature effect techniques Partial
Dependence Plots~\cite{Friedman2001}, Marginal Plots and
ALE~\cite{Apley2020}. Another class of global explanation techniques
measures the interaction~\cite{Friedman2008} between features. Feature
interaction quantifies to what extent the effect of two variables on
the output comes is because of their combination.~\cite{Friedman2008}
proposed a set of appropriate visualizations for such
interactions. The generalization of feature effect and variable
interactions is functional decomposition~\cite{Molnar2021}, that
decomposes the black-box function into a set of simpler ones that may
include more than two features.
