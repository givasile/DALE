OOD sampling is the source of failure in many explainability methods
that perturb features~\citep{Hooker2021}. ALE is vulnerable to OOD
sampling when the bin length is relatively big, or, equivalently, when
the number of bins (hyperparameter \(K\)) is relatively small. We use
the word \textit{relatively} to indicate that the threshold for
characterizing a bin as big/small depends on the properties of the
black-box function, i.e., how quickly it diverges outside of the data
manifold. ML models learn to map \( \xb \rightarrow y \) only in the
manifold of the data generating distribution \(\Xb\). Therefore, the
black-box function \(f\) can take any arbitrary form away from \(\Xb\)
without any increase in the training loss. On the other hand, when a
limited number of samples is available, it maybe necessary to lower
\(K\) to ensure a robust estimation of the mean effect. An end-to-end
experimentation on the effect of OOD will be provided in Case 2 of
Section~\ref{sec:5-1-artificial-experiments}.

In Figure~\Ref{fig:example-different-bins} we illustrate a small
example where the underlying black-box function \(f\) has different
behavior on the data generating distribution and away from it. As can
be seen in Figure~\ref{fig:example-different-bins}(a), we set the
black-box function to be \(f = x_1x_2\) inside \(|x_1-x_2| < 0.5\) and
to rapidly diverge outside of the region. The first feature follows a
uniform distribution, i.e. \(x_1 \sim U(0,10)\), and for the second
feature \(x_2=x_1\). The local effect of \(x_1\) is
\(f_1(\xb) = x_2 \). Spliting in K bins, the first bin is in
\( [0, \frac{10}{K} ) \), therefore, the ground truth bin effect is
\(\int_0^{10/K} \E_{x_2|z}\left[f_1(\xb)\right]\partial z =
\frac{5}{K}\). In Figure~\Ref{fig:example-different-bins}(b), we
observe that as the bin-length becomes bigger (smaller K), DALE
approximates the effect perfectly, whereas, ALE fails due to OOD
sampling. This happens because in the ALE approximation of
Eq.~\eqref{eq:ALE_appr}, the bin limits \(z_{k-1}, z_k\) fall outside
of the region \(|x_1-x_2| < 0.5\).


% For \(x_1\), the local effect is
% \(\frac{\partial f}{\partial x_1} = x_2\).  Spliting in K bins, the
% expected value of the local effect in the first bin is
% \(\frac{10}{K}\).

% We argue that grid density is a hyperparameter that should not lead to
% different local effect estimations. Instead, it should only affect the
% resolution of the feature effect plot, as is the case with DALE. The
% user may select a narrow grid if interested only in the general trend
% of the feature effect (coarse-grain details) without risking deviating
% from the data manifold.


\begin{figure}[h]
  \centering
  \resizebox{.35\columnwidth}{!}{\input{./images/OOD-1.tex}}
  \resizebox{.35\columnwidth}{!}{\input{./images/OOD-2.tex}}
  \caption[Example comparison]{(Left) The black-box function \(f\) of
    Section~\ref{sec:4-3-robustness}. (Right) Estimation of the local
    effect of the first bin for DALE and ALE, for varying number of
    bins \(K\).}
  \label{fig:example-different-bins}
\end{figure}
