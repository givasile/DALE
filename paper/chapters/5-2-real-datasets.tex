In this section, we test our approximation on the Bike-Sharing
Dataset~\cite{BikeSharing}.~\footnote{It is dataset drawn from the
  Capital Bikeshare system (Washington D.C., USA) over the period
  2011-2012. The dataset can be found
  \href{https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip}{here}}
The Bike-Sharing Dataset is chosen as the main illustration example in
the original ALE paper, therefore it was considered appropriate for
comparisons. The dataset contains the bike rentals for almost every
hour over the period 2011 and 2012. The dataset contains 14 features,
which we denote as \( X_{\mathtt{<feature\_name>}} \). We select the
11 features that are relevant to the prediction task. Most of the
features are measurements of the environmental conditions, e.g.
\(X_{\mathtt{month}}\), \(X_{\mathtt{hour}}\),
\(X_{\mathtt{temperature}}\), \(X_{\mathtt{humidity}}\),
\(X_{\mathtt{windspeed}}\), while some others inform us about the
day-type, e.g. whether we refer to a working-day
\(X_{\mathtt{workingday}}\). The target value \( Y_{\mathtt{count}}\)
is the bike rentals per hour, which has mean value
\(\mu_{\mathtt{count}} = 189\) and standard deviation
\(\sigma_{\mathtt{count}} = 181\). We train a deep fully-connected
Neural Network with 6 hidden layers and \(711681\) parameters. We
train the model for \(20\) epochs, using the Adam optimizer with
learning rate 0.01. The model achieves a mean absolute error on the
test of about \(38\) counts.

\paragraph{Efficiency} For comparing the efficiency, we measure the
execution time of DALE and ALE for a variable number of features. We
present the results in Table~\ref{tab:bike-sharing-efficiency}. We
confirm that DALE can compute the feature effect for all features in
almost constant time wrt \(D\). In contrast, ALE scales linearly wrt
\(D\) which leads to an execution time of over \(10\) seconds.

\begin{table}
  \caption{Measurements of the execution time in seconds, for DALE and ALE approximation
    on the Bike Sharing dataset.}
  \label{tab:bike-sharing-efficiency}
  \centering
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
    \multicolumn{12}{c}{Efficiency on Bike-Sharing Dataset} \\
    \hline\hline
    & \multicolumn{11}{|c}{Number of Features} \\
    \hline
    & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\
    \hline
    \( \dale \) & 1.17 & \textbf{1.19} & \textbf{1.22} & \textbf{1.24} & \textbf{1.27} & \textbf{1.30} & \textbf{1.36} & \textbf{1.32} & \textbf{1.33} & \textbf{1.37} & \textbf{1.39} \\
    \hline
    \( \alep \) & \textbf{0.85} & 1.78 & 2.69 & 3.66 & 4.64 & 5.64 & 6.85 & 7.73 & 8.86 & 9.9 & 10.9 \\
    \hline
  \end{tabular}
\end{table}

\paragraph{Accuracy}

In the case of the Bike-Sharing, it is infeasible compare to compute
the ground-truth ALE. We have lack of knowledge about the
data-generating distribution and the dimensionality of the problem
\(D=11\) is prohibitive for applying numerical integration on
eq.~\eqref{eq:ALE}. However, given the fact that the dataset has a
large number of instances, DALE and ALE provide very similar
approximations for large \(K\), e.g. (\(K=200\)), and we can treat
these approximations as the ground-truth effect. In
Figure~\ref{fig:bike-sharing-comparison}, we illustrate the feature
effect for three features.

For all feature features, except \(X_{\mathtt{hour}}\), lowering the
number of bins \(K\) does not significantly impacts the approximation,
since these features change slowly wrt the feature value. An exception
is feature \(X_{\mathtt{hour}}\). In this case, the
\(f_{\mathtt{DALE}}\) approximation remains accurate when lowering the
number of bins \(K\) (Fig.~\ref{fig:bike-sharing-feature-3}(b)), while
\(\hat{f}_{\mathtt{ALE}}\) deteriorates significantly
(Fig.~\ref{fig:bike-sharing-feature-3}(c)). In
Table~\ref{tab:bike-sharing-accuracy} we evaluate the feature effect
of \(X_{\mathtt{hour}}\) for different number of bins \(K\). The
ground-truth effect has been computed for \(K=200\). We observe that
NMSE remains low in DALE for all \(K\), while for ALE it rapidly
increases. This is due to OOD sampling that occurs when the bin size
becomes large.


\begin{figure}[h]
  \centering
  \resizebox{.3\columnwidth}{!}{\input{./images/bike-dataset-fe-1.tex}}
  \resizebox{.3\columnwidth}{!}{\input{./images/bike-dataset-fe-8.tex}}
  \resizebox{.3\columnwidth}{!}{\input{./images/bike-dataset-fe-9.tex}}
  \caption{DALE and ALE feature effect plots with \(K=200\) for:
    \(X_{\texttt{month}}\), \(X_{\mathtt{atemp}}\),\(X_{\mathtt{hum}}\).}
  \label{fig:bike-sharing-comparison}
\end{figure}


\begin{figure}[h]
  \centering
    \resizebox{.3\columnwidth}{!}{\input{./images/bike-dataset-fe-2.tex}}
    \resizebox{.3\columnwidth}{!}{\input{./images/bike-dataset-dale-comparison.tex}}
    \resizebox{.3\columnwidth}{!}{\input{./images/bike-dataset-ale-comparison.tex}}
    \caption{Feature effect plots on \(X_{\texttt{hour}}\): (Left)
      DALE vs ALE for \(K=200\). (Center) DALE plots for
      \(K = \{25, 50, 100\}\). (Right) ALE plots for
      \(K = \{25, 50, 100\}\)}
  \label{fig:bike-sharing-feature-3}
\end{figure}


\begin{table}
  \caption{Evaluation of DALE and ALE approximation when lowering the
    number of bins \(K\). The ground-truth effect has been computed
    for \(K=200\).}
  \label{tab:bike-sharing-accuracy}
  \centering
  \begin{tabular}{c|c|c|c|c|c}
    \multicolumn{6}{c}{Accuracy on Bike-Sharing Dataset - Feature \(X_{\mathtt{hour}}\)} \\
    \hline \hline
    & & \multicolumn{4}{|c}{Number of bins} \\
    \hline
    & & 100 & 50 & 25 & 15 \\
    \hline
    \hline
    \multirow{2}{*}{\(\mathtt{NMSE}\)} & \(\dale\) & \textbf{0.007} & \textbf{0.01} & \textbf{0.03} & \textbf{0.09} \\
    & \(\alep\) & 0.04 & 0.43 & 0.79 & 0.83 \\
    \hline
  \end{tabular}
\end{table}
