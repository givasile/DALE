DALE approximation has significant computational advantages,
especially in cases of high-dimensional input space. For estimating
the feature effect of all features, our approach processes \(N\) data
points, namely, the examples of the training set. In contrast, ALE
approximation generates and processes \(2 \cdot N \cdot D\)
artificial data points. The difference by a factor of \(D\) has major
implications in the computational complexity and the memory
requirements. Our method scales nicely in problems with high
dimensionality, as is the case in most Deep Learning setups. Our
approach is built on the computation of the Jacobian matrix,

\begin{equation} \Jac =
  \begin{bmatrix} \nabla_{\xb}f(\xb^1) \\ \vdots \\ \nabla_{\xb}f(\xb^N)
  \end{bmatrix} =
  \begin{bmatrix} f_1(\xb^1) & \dots & f_D(\xb^1)\\ \vdots & \ddots & \vdots \\ f_1(\xb^N) & \dots & f_D(\xb^N)
  \end{bmatrix}
\label{eq:jacobian}
\end{equation}

\noindent
where, as before, \( f_s(\xb^i) \) is the partial derivative of the
\(s\)-th feature evaluated at the \(i\)-th training point. Automatic
differentiation enables the computation of the gradients w.r.t. all
features in a single pass. Computing the gradient vector for a
training example \(\xb^i\) w.r.t all features
\( \nabla_{\xb}f(\xb^i) = [f_1(\xb), \ldots, f_D(\xb)] \) is
computationally equivalent to evaluating \(f(\xb^i)\). Based on this
observation, computing the whole Jacobian matrix costs
\(\mathcal{O}(N)\). In constrast, in ALE, the evaluation of \(f\) for
\(N \cdot D\) times costs \(\mathcal{O}(N \cdot D)\). Our method,
also, takes advantage of all existing automatic differentiation
frameworks which are optimized for computing the gradients
efficiently.\footnote{For example, the computation of that Jacobian
  can be done in a single command using Tensorflow
  \( \mathtt{tf.GradientTape.jacobian(predictions, X)} \) and Pytorch
  \( \mathtt{torch.autograd.functional.jacobian(f, X)} \)} In
Algorithm~\ref{alg:dale}, we present DALE in an algorithm form. The
algorithm needs as input: (a) the black-box function \(f\), (b) the
derivative of \(\nabla_{\mathbf{x}} f \) and (c) the dataset
\( \mathbf{X} \).\footnote{Technically, having access to
  \(\nabla_{\mathbf{x}} f \) is not a prerequisite, since the partial
  derivative \(\frac{\partial f}{\partial x}\) can be approximated
  numerically, with finite differences. However, in this case, the
  computational advantages are canceled.} The parameter \( K \)
defines the resolution of the DALE plot. The algorithm returns a
matrix \(\mathbf{A}\), where the cell \(\mathbf{A}_{s,j}\) contains
the effect of the \(j\)-th bin of the \(s\)-th feature, i.e.,
\(f_{\mathtt{DALE}}^s(x) = \mathbf{A}_{s,k_x} \). Steps 3-5 iterate
over each attribute, therefore these steps have complexity
\(\mathcal{O}(N \cdot D)\). However these steps involve relatively
cheap operations (allocation, averaging and aggregation) in comparison
with the computation of the Jacobian matrix. Finally, with matrix
\(\mathbf{A}\) computed, evaluating \(f_{\mathtt{DALE}}(x)\) requires
only locating the bin \(k_x\) that \( x \) belongs to.

\begin{algorithm}[h]
\caption{DALE aproximation}
\label{alg:dale}
\textbf{Input}: \( f, \nabla_{\mathbf{x}} f, \mathbf{X} \) \\
\textbf{Parameter}: \( K \) \\
\textbf{Output}: \(\mathbf{A}\)
\begin{algorithmic}[1] %[1] enables line numbers
\STATE Compute the Jacobian \(\Jac\) of Eq. \eqref{eq:jacobian}
\FOR{\(s = 1, \ldots , D\)}
\STATE Allocate points \( \Rightarrow \mathcal{S}_k \forall k \)
\STATE Estimate local effect \( \Rightarrow \hat{\mu}_k^s \forall k\) of Eq.~\eqref{eq:DALE}
\STATE Aggregate \( \Rightarrow \mathbf{A}_{s,j} = \Delta x\sum_{k=1}^{j} \hat{\mu}_k^s, j =  1, \ldots, K \)
\ENDFOR
\STATE \textbf{return} \(\mathbf{A}\) \textbf{||} Note that \( f_{\mathtt{DALE}}(x) = \mathbf{A}_{s,k_x} \)
\end{algorithmic}
\end{algorithm}
