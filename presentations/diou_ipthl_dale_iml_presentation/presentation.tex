%%
%% Slides for Applications of Artificial Intelligence course
%% Department of Informatics and Telematics,
%% Harokopio University of Athens, Greece.
%%
%% Author: Christos Diou

\input{preamble}

%% \Large IML and DALE: Introduction to Interpretable Machine
%% Learning \footnote{Some ideas and content from  Christoph Molnar's book, ``Interpretable Machine Learning book'',
%% \url{https://christophm.github.io/interpretable-ml-book/}} and Differential
%% Accumulated Local Effects\footnote{based on work with colleagues V. Gkolemis
%% and T. Dalamagas, \url{https://arxiv.org/abs/2210.04542}}

\title{\Large{IML and DALE: Introduction to Interpretable Machine
    Learning and Differential Accumulated Local Effects}}

\AtBeginSection[]{
  \begin{frame}{Contents}
    \small \tableofcontents[currentsection, hideothersubsections]
  \end{frame} 
}

\begin{document}

\frame{\titlepage}

% Use label=current to render only one slide

\section{Introduction}

\begin{frame}[plain,c]
  \Large Short introduction to interpretable machine learning \footnote{Some
  ideas and content from Christoph Molnar's book, ``Interpretable Machine Learning'' (IML book),
  \url{https://christophm.github.io/interpretable-ml-book/}}
\end{frame}

\begin{frame}
  \frametitle{Hypothetical (?) scenarios}

  \begin{itemize}
  \item<1-> The computer vision subsystem of an autonomous vehicle leads the
    vehicle to take a left turn, in front of a car moving in the opposite direction\footnote{\url{https://www.theguardian.com/technology/2022/dec/22/tesla-crash-full-self-driving-mode-san-francisco}}
  \item<2-> The credit assessment system leads to the rejection of an
    application for a loan - the client suspects racial bias\footnote{\url{https://www.technologyreview.com/2021/06/17/1026519/racial-bias-noisy-data-credit-scores-mortgage-loans-fairness-machine-learning/}}
  \item<3-> A model that assesses the risk of future criminal offenses (and
    used for decisions on parole sentences) is biased against black
    prisoners\footnote{\url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Questions}
  \begin{itemize}
  \item Why did a model make a specific decision?
  \item What could we change so that the model will make a different decision?
  \item Can we summarize and predict the model's behavior?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Interpretability of Machine Learning Models}
  Qualitative definitions:
  \begin{itemize}
  \item ``Interpretability is the degree to which a human can understand the
    cause of a decision'' \footnote{Miller, Tim. ``Explanation in artificial
    intelligence: Insights from the social sciences.'' arXiv Preprint
    arXiv:1706.07269. (2017)}
  \item ``Interpretability is the degree to which a human can consistently
    predict the model’s result''\footnote{Kim, Been, Rajiv Khanna, and
    Oluwasanmi O. Koyejo. ``Examples are not enough, learn to criticize!
    Criticism for interpretability.'' Advances in Neural Information Processing
    Systems (2016).} 
  \item ``Extraction of relevant knowledge from a machine-learning model
    concerning relationships either contained in data or learned by the
    model''\footnote{Murdoch, W. J., Singh, C., Kumbier, K., Abbasi-Asl, R. and
    Yu, B. ``Definitions, methods, and applications in interpretable machine
    learning.'' Proceedings of the National Academy of Sciences, 116(44),
    22071-22080. (2019)} 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Generalization}
  \begin{onlyenv}<1>
    To get some intuition, consider a process that produces output
    $y$ for scalar input $x$
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_1_reality.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<2>
    Unfortunately this process is uknown to us, but we can sample a small
    number of input - output pairs. During sampling, we have a small amount of
    measurement noise (same if the process is stochastic)
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_2_sampling.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<3>
    Our goal is to model the process using the available samples (regression)
    \vspace{1cm}\\
  \end{onlyenv}
  \begin{onlyenv}<4>
    Linear model:
    \begin{equation*}
      y = w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_3_linear.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<5>
    2$^{nd}$ degree polynomial:
    \begin{equation*}
      y = w_2\cdot x^2 + w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_4_quadratic.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<6>
    3$^{rd}$ degree polynomial:
    \begin{equation*}
      y = w_3\cdot x^3 + w_2\cdot x^2 + w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_5_3d.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<7>
    9$^{th}$ degree polynomial
    \begin{equation*}
      y = \sum_{i=0}^{9}w_i\cdot x^{i}
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_6_9d.pgf}
      }
    \end{center}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{Problem diagnosis}

  \begin{itemize}
  \item The model behavior is immediately understood by the shape of the function
  \item Overfitting is immediately diagnosed
  \item But what happens if we have multiple dimensions, $p$, making
    visualization impossible?
    \begin{itemize}
    \item We often have tens or hundreds of features
    \item Images and signals: Several thousands of input dimensions
    \end{itemize}
  \end{itemize}
  %\vspace{1cm}
  %\uncover<2>{\obf{Γιατί μας ενδιαφέρει να καταλάβουμε τη συμπεριφορά ενός μοντέλου;}}
\end{frame}

\begin{frame}
  \frametitle{Taxonomy of interpretability methods}
  \begin{figure}
    \includegraphics[width=.8\textwidth]{taxonomy_speith}
    \caption{\footnotesize Timo Speith, ``A Review of Taxonomies of Explainable
      Artificial Intelligence (XAI) Methods''. In 2022 ACM Conference on
      Fairness, Accountability, and Transparency (FAccT '22), 2022}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Interpretable models (ante-hoc)}
  \begin{onlyenv}<1>
    \begin{itemize}
    \item Some models afford explanations
    \item Examples, (generalized) linear models, decision trees, $k$-NN
    \item Example: Linear regression
      \begin{equation*}
        \hat{y} = w_1x_1 + \dotsc + w_px_p + b
      \end{equation*}
    \end{itemize}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{itemize}
    \item Result in the bike sharing dataset (model weights)
      \begin{equation*}
        \hat{y} = w_1x_1 + \dotsc + w_px_p + b
      \end{equation*}
    \end{itemize}
    \begin{center}
      \begin{figure}
        \includegraphics[width=.6\textwidth]{lr_weights}
        \caption{\footnotesize C. Molnar, IML book}
      \end{figure}
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<3>
    \begin{itemize}
    \item Feature effects (visualization)
      \begin{equation}
        effect_j^{(i)} = w_jx_j^{(i)}
      \end{equation}
    \end{itemize}
    \begin{center}
      \begin{figure}
        \includegraphics[width=.5\textwidth]{lr_effects}
        \caption{\footnotesize C. Molnar, IML book}
      \end{figure}
    \end{center}
  \end{onlyenv}
\end{frame}

\section{Local, model-agnostic methods}

\begin{frame}
  \frametitle{Local, model agnostic methods}
  \begin{figure}
    \includegraphics[width=.8\textwidth]{taxonomy_speith}
    \caption{\footnotesize Timo Speith, ``A Review of Taxonomies of Explainable
      Artificial Intelligence (XAI) Methods''. In 2022 ACM Conference on
      Fairness, Accountability, and Transparency (FAccT '22), 2022}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Goal}
  \begin{itemize}
  \item Most models do not afford explanations 
    \begin{itemize}
    \item \orange{we cannot explain them by looking at their parameters}
    \item \orange{we handle these as ``black boxes''}
    \end{itemize}
  \item In this case we apply general interpretability methods
  \item \obf{Local}: Interpret the model's output for a particular input instance
  \item \obf{Global}: Provide a general interpretation of the model's behavior
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{LIME - Local Interpretable Model-agnostic
    Explanations\footnote{Ribeiro, Marco Tulio, Sameer Singh, and Carlos
    Guestrin. ``Why should I trust you?: Explaining the predictions of any
    classifier.'' Proceedings of the 22nd ACM SIGKDD international conference
    on knowledge discovery and data mining. ACM (2016)}}
  \begin{onlyenv}<1>
    \begin{itemize}
    \item Idea:
      \begin{itemize}
      \item Train an interpretable model with samples in the neighborhood of the
        target instance, weighted by their proximity
      \end{itemize}
      \begin{equation*}
        \text{explanation(x)} = \arg\min\limits_{g\in G}L(f, g, \pi_x) + \Omega(g)
      \end{equation*}
      \begin{itemize}
      \item $g$ is the interpretable model
      \item $\pi_x$ is the weighting function (e.g., a radial basis function
        kernel)
      \item $\Omega(g)$ is a regularizer for $g$ (e.g., LASSO, or limit on the
        number of features)
      \end{itemize}
    \end{itemize}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{itemize}
    \item Idea (visualization)
    \end{itemize}
    \begin{figure}
      \includegraphics[width=.4\textwidth]{lime_idea}
      \caption{\footnotesize Ribeiro et al, 2016}
    \end{figure}
  \end{onlyenv}
  \begin{onlyenv}<3>
    \begin{itemize}
    \item Application on text data
    \end{itemize}
    \begin{figure}
      \includegraphics[width=.6\textwidth]{lime_atheism}
      \caption{\footnotesize Ribeiro et al, 2016}
    \end{figure}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{LIME with images}
  \begin{itemize}
  \item Instead of features, we use superpixels (e.g., extracted via quick shift)
  \item We obtain samples by ``removing'' superpixels (e.g., by replacing their
    pixels with medium gray)
  \end{itemize}
  \begin{figure}
    \includegraphics[width=.8\textwidth]{LIME}
    \caption{\footnotesize Ribeiro et al., 2016}
  \end{figure}
\end{frame}

% TODO
\begin{frame}
  \frametitle{SHAP}
  \begin{itemize}
  \item Let $\phi_j$ be the feature attribution of the $j$-th feature
  \item Then,
    \begin{equation*}
      g(z') = \phi_0 + \sum\limits_{j = 1}^{M}\phi_jz_j'
    \end{equation*}
    \begin{itemize}
    \item $z' \in \left\{0, 1\right\}^M$ (all 1's for the target instance)
    \item General definition - applies to LIME too!
    \end{itemize}
  \end{itemize}
  \begin{figure}
    \includegraphics[width=.8\textwidth]{shap_attributions}
    \caption{\footnotesize S.M. Lundberg and S.I Lee. A unified approach to
        interpreting model predictions. Advances in neural information
        processing systems, 2017}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Kernel SHAP - procedure}
  \begin{onlyenv}<1>
  \begin{enumerate}
  \item Sample $K$ binary vectors $z'_k \in \left\{0, 1\right\}^M$
  \item Get a value $x'$ by using mapping function $h_x(z'_k)$
    \begin{itemize}
      \item Get value of $x_j$ if $z'_j = 1$, get the value from another
        randomly selected dataset sample if $z'_j = 0$
    \end{itemize}
  \item Get prediction $\hat{f}(h_x(z'_k))$
  \item Compute weight using SHAP kernel (which has some nice properties - see paper)
  \item Fit linear model
  \item $\phi_k$ are the linear model coefficients
  \end{enumerate}
  \end{onlyenv}
  \begin{onlyenv}<2>
      \begin{figure}
    \includegraphics[width=.8\textwidth]{shap_example}
    \caption{\footnotesize C. Molnar, IML book}
  \end{figure}
  \end{onlyenv}
\end{frame}

\begin{frame}
    \frametitle{SHAP for images}
  \begin{itemize}
  \item Similar idea with LIME, apply SHAP on superpixels
    \begin{figure}
      \includegraphics[width=.6\textwidth]{shap-superpixel}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{itemize}
\end{frame}

\section{Methods for CNN interpretation}

\begin{frame}
  \frametitle{Visualization of extracted features}
  \begin{figure}
    \includegraphics[width=\textwidth]{features}
    \caption{\footnotesize \url{https://distill.pub/2017/feature-visualization/}}
  \end{figure}
  \begin{itemize}
  \item These images maximize the activation of specific filters of an
    Inception-V1 network at different depths
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Pixel attribution / Saliency maps}
  Methods that visualize the contribution of different areas of an image in the
  final decision
  \begin{itemize}
  \item Occlusion- or perturbation-based methods)
    \begin{itemize}
    \item SHAP / LIME belong in this category
    \end{itemize}
  \item Gradient-based methods
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Saliency maps (vanilla gradient)}
  \begin{enumerate}
  \item Forward pass of the input image, $I_0$
  \item Compute the derivative of the output/class of interest $S_c$, with
    respect to the input
    \begin{equation*}
      E_{grad}(I_0) = \frac{\partial S_c}{\partial I}\Bigr|_{I=I_0}
    \end{equation*}
  \item visualize the resulting image
  \end{enumerate}
  Question: How to handle $ReLU$;
  \begin{align*}
    X_{n+1} &= \max(0, X_n)\\
    \frac{\partial f}{\partial X_n} &= \frac{\partial f}{\partial X_{n+1}}\mathbf{I}(X_n > 0)
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Saturation}
  \begin{figure}
    \includegraphics[width=.6\textwidth]{saturation}
    \caption{\footnotesize A. Shrikumar, P. Greenside, and A.
      Kundaje. ``Learning important features through propagating activation
      differences.'' Proceedings of the 34th International Conference on
      Machine Learning-Volume 70. JMLR. org, (2017).}
  \end{figure}
  \begin{itemize}
  \item Variations of Saliency Maps: DeconvNets, Guided Backpropagation
    \begin{itemize}
    \item Different handling of $ReLU$, do not completely overcome the
      saturation problem
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{GradCAM}
  Different approach; starting from the output of the next to last layer
  (before softmax) for class $c$, and for activations of features $A^{k}$ of a layer $A$
  (often the last conv layer)
  \begin{enumerate}
  \item Apply global average pooling on derivatives
    \begin{equation*}
      a_{k}^{c} = \frac{1}{Z}\sum\limits_i\sum\limits_j\frac{\partial
        y^{c}}{\partial A^{k}_{i, j}}
    \end{equation*}
    \begin{itemize}
    \item Coefficients $a_{k}^{c}$ quantify the importance of layer $k$ for
      detecting class $c$
    \end{itemize}
  \item Use weighted sum of activations to produce the final visualization
    \begin{equation*}
      L_{GradCAM}^{c} = ReLU\left(\sum\limits_ka_kA^{k}\right)
    \end{equation*}
  \end{enumerate}
\end{frame}

\begin{frame}
  \frametitle{Example - initial images}
  \begin{figure}
    \includegraphics[width=.6\textwidth]{original-images}
    \caption{\footnotesize C. Molnar, IML book}
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{Example - gradient-based interpretations}
  \begin{figure}
    \includegraphics[width=.6\textwidth]{grad}
    \caption{\footnotesize C. Molnar, IML book}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Problems - drawbacks (1)}
  \begin{itemize}
  \item Evaluation of these methods is commonly qualitative - we don't know if
    the interpretation is correct
  \item It has been demonstrated that these methods are very sensitive
    \footnote{A. Ghorbani, A. Abid, and J. Zou. ``Interpretation of
    neural networks is fragile.'' Proceedings of the AAAI Conference on
    Artificial Intelligence. Vol. 33. 2019.}
    \begin{itemize}
    \item \orange{Very small changes of the input can lead to the same output but
      completely different interpretation}
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Problems - drawbacks (2)}
  \begin{itemize}
  \item It has also been shown that some of these methods are unreliable\footnote{P-J Kindermans, S. Hooker, J. Adebayo, M. Alber,
    K. T. Sch\"{u}tt,  S. D\"{a}hne, D. Erhan and B. Kim. ``The (un)
    reliability of saliency methods.'' In Explainable AI: Interpreting,
    Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham
    (2019)}
    \begin{itemize}
    \item \orange{Adding constant pixel offset and changing the bias term of
      the first layer leads to the same predictions and derivatives, but to
      different interpretations}
    \end{itemize}
  \item It has finally been show that often these methods do not depend on the
    model or the data (and are therefore not useful for interpretation), similarly to an edge
    detector\footnote{J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt
    and B. Kim. ``Sanity checks for saliency maps.'' arXiv preprint
    arXiv:1810.03292 (2018)}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Example - similarity to edge detectors}
  \begin{figure}
    \includegraphics[width=.8\textwidth]{edge_detectors}
    \caption{\footnotesize Adebayo et al, 2018}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Example - randomization test}
  \begin{figure}
    \includegraphics[width=.8\textwidth]{randomization_test}
    \caption{\footnotesize Adebayo et al, 2018}
  \end{figure}
\end{frame}

\section{Global, model agnostic methods}

\begin{frame}
  \frametitle{Goal}
  \begin{itemize}
  \item Our aim is to produce interpretations that describe the model's
    behavior as a whole 
  \item We focus on tabular data, and the result is usually a plot
  \end{itemize}
  \begin{figure}
    \includegraphics[width=.6\textwidth]{taxonomy_speith}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Feature effect methods}
  \begin{itemize}
  \item \(x_s \rightarrow \) feature of interest, \(\Vx_c \rightarrow\) other features
  \item How do we isolate the effect of \(x_s\)?
    %% \item Προβλήματα:
    %%   \begin{itemize}
    %%   \item Συσχετισμένα χαρ/κά
    %%   \item Η \(f\) έχει μάθει σύνθετες αλληλεπιδράσεις 
    %%   \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Partial Dependence Plots (PDP)}
  \begin{onlyenv}<1>
    \begin{itemize}
    \item Proposed by J. Friedman on 2001\footnote{J. Friedman. ``Greedy
    function approximation: A gradient boosting machine.'' Annals of statistics
    (2001): 1189-1232} and is the marginal \obf{effect} of a feature to the
      model output:
      \begin{equation*}
        \hat{f_s(x_s)} = E_{X_c}\left[\hat{f}(x_s, X_c)\right] =
        \int\hat{f}(x_s, X_c)d\mathbb{P}(X_c)
      \end{equation*}
      where $x_s$ is the feature whose effect we wish to compute and $X_c$ is a
      random variable corresponding to the rest of the model's features
    \item Computation:
      \begin{equation*}
        \hat{f}_s(x_s) = \frac{1}{n}\sum\limits_{i=1}^{n}\hat{f}(x_s, \Vx_c^{(i)})
      \end{equation*}
    \end{itemize}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \obf{Example 1 (continuous):}
    \begin{figure}
      \includegraphics[width=.6\textwidth]{pdp-bike-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
  \begin{onlyenv}<3>
    \obf{Example 2 (categorical):}
    \begin{figure}
      \includegraphics[width=.6\textwidth]{pdp-bike-cat-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{Issues with PDPs}
  \begin{onlyenv}<1>
    \begin{itemize}
    \item Correlated features
      \begin{itemize}
      \item Example: $\text{price} = f(\text{num\_rooms}, \text{area})$
      \item To compute the effect of area for 40 $m^2$ we will use value 10 for
        the number of rooms (uncrealistic)
      \item As a result, we have a wrong estimation of the feature effect
      \end{itemize}
    \item Heterogeneous feature effects may be hidden by the use of average values
    \end{itemize}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{figure}
      \includegraphics[width=.6\textwidth]{aleplot-motivation1-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{MPlots}

  \begin{onlyenv}<1>
    We use the value of $x_s$ as a condition
    \begin{itemize}
    \item \(\Vx_c|x_s\): \(f(x_s) = \mathbb{E}_{\Vx_c|x_s}[f(x_s, \Vx_c)]\)
    \end{itemize}
  \end{onlyenv}
  \begin{onlyenv}<2>
    In the previous example
    \begin{figure}
      \includegraphics[width=.6\textwidth]{aleplot-motivation2-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{Problems with M-Plots}
  \begin{itemize}
  \item The calculated effects result from the combination of all (correlated) features
  \item Real effect: \(x_{\mathtt{age}} = 50 \rightarrow 10\), \(x_{\mathtt{years\_contraceptives}} = 20 \rightarrow 10\)
  \item MPlot may estimate an effect of 17 for both
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Accumulated Local Effects (ALE)\footnote{D. Apley and
    J. Zhu. ``Visualizing the effects of predictor variables in black box
    supervised learning models.'' Journal of the Royal Statistical Society:
    Series B (Statistical Methodology) 82.4 (2020): 1059-1086.}}

  \begin{itemize}
  \item Resolves problems that result from the feature correlation by computing
    differences over a (small) window
  \item \(f(x_s) = \int_{x_{min}}^{x_s}\mathbb{E}_{\Vx_c|z}[ \frac{\partial f}{\partial x_s}(z, \Vx_c)] \partial z\)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{ALE approximation}
  ALE definition: \( f(x_s) = \int_{x_{s, min}}^{x_s}\mathbb{E}_{\Vx_c|z}[ \frac{\partial f}{\partial x_s}(z, \Vx_c)] \partial z \)
  % \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  ALE approximation: \(f(x_s) = \sum\limits_{k=1}^{k_x}
  \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\Vx^i \in \mathcal{S}_k}
    \underbrace{[f(z_k, \Vx^i_c) - f(z_{k-1}, \Vx^i_c)]}_{\text{point
        effect}}}_{\text{bin effect}} \) 

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figures/ale_bins_iml.png}
    \caption{\footnotesize C. Molnar, IML book}
  \end{figure}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  The number of bins (parameter \(K\)) is critical!
\end{frame}

\begin{frame}
  \frametitle{ALE plots - examples}
  \begin{onlyenv}<1>
    \begin{figure}
      \includegraphics[width=.6\textwidth]{ale-bike-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{figure}
      \includegraphics[width=.6\textwidth]{ale-bike-cat-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
\end{frame}

\section{DALE}

\begin{frame}[plain,c]
  \Large{V. Gkolemis, T. Dalamagas, C. Diou, ``DALE: Differential Accumulated
  Local Effects for efficient and accurate global explanations'', ACML, 2022}

  \vspace{1cm}
  Most of the work done by this guy:\\
  \begin{figure}
    \includegraphics[width=.2\textwidth]{gkolemis}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{ALE approximation - weaknesses}

  \begin{equation*}
    f(x_s) = \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\xb^i
        \in \mathcal{S}_k} \underbrace{[f(z_k, \Vx^i_c) - f(z_{k-1},
          \Vx^i_c)]}_{\text{point effect}}}_{\text{bin effect}}
  \end{equation*}

  \begin{itemize}
  \item Point Effect \(\Rightarrow\) evaluation \alert{at bin limits}
    \begin{itemize}
    \item 2 evaluations of \(f\) per point \( \rightarrow \) slow
    \item change bin limits, pay again \(2*N\) evaluations of \(f\) \( \rightarrow\) restrictive
    \item broad bins may create out of distribution (OOD) samples \( \rightarrow\) not-robust in wide bins
    \end{itemize}
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Our proposal: Differential ALE}
  \begin{equation*}
    f(x_s) = \Delta x \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|}
      \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial f(x_s^i, \Vx^i_c)}{\partial
            x_s}]}_{\text{\alert{point effect}}}}_{\text{bin effect}}
  \end{equation*}

  \begin{itemize}
  \item Point Effect \(\Rightarrow\) evaluation \alert{on instances}
    \begin{itemize}
    \item Fast \( \rightarrow \) use of auto-differentiation, all derivatives in a single pass
    \item Versatile \( \rightarrow\) point effects computed once, change bins without cost
    \item Secure \( \rightarrow\) does not create artificial instances
    \end{itemize}
  \end{itemize}

  For \alert{differentiable} models, DALE resolves ALE weaknesses
\end{frame}



% chapter 1
\section{DALE vs ALE}

\subsection{Dale is faster and versatile}

\begin{frame}
  \frametitle{DALE is faster and versatile - theory}
  \[f(x_s) = \Delta x \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial f(x_s^i, \Vx^i_c)}{\partial x_s}]}_{\text{point effect}}}_{\text{bin effect}} \]

  \begin{itemize}
  \item Faster
    \begin{itemize}
    \item gradients wrt all features \(\nabla_{\Vx} f(\Vx^i)\) in a single pass
    \item auto-differentiation must be available (deep learning)
    \end{itemize}
  \item Versatile
    \begin{itemize}
    \item Change bin limits, with near zero computational cost
    \end{itemize}

  \end{itemize}
  DALE is faster and allows redefining bin-limits
\end{frame}

\begin{frame}
  \frametitle{DALE is faster and versatile - Experiments}
  \begin{figure}[h]
    \centering
    \resizebox{.4\columnwidth}{!}{\input{figures/case-1-plot-1.tex}}
    \resizebox{.43\columnwidth}{!}{\input{figures/case-1-plot-2.tex}}
    \caption[Case-1-fig-1]{Light setup; small dataset \((N=10^2\) instances), light \(f\). Heavy setup; big dataset (\(N=10^5\) instances), heavy \(f\)}
    \label{fig:case-1-plots-1}
  \end{figure}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  DALE considerably accelerates the estimation
\end{frame}


\subsection{DALE is more Accurate}

\begin{frame}
  \frametitle{DALE uses on-distribution samples - Theory}
  \[f(x_s) = \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|}
    \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial
          f(x_s^i, \Vx^i_c)}{\partial x_s}]}_{\text{point
        effect}}}_{\text{bin effect}} \]

  \begin{itemize}
  \item point effect \alert{independent} of bin limits
    \begin{itemize}
    \item \(\frac{\partial f(x_s^i, \Vx^i_c)}{\partial x_s}\)
      computed on real instances \(\Vx^i = (x_s^i, \Vx_c^i)\)
    \end{itemize}
  \item bin limits affect only the \alert{resolution} of the plot
    \begin{itemize}
    \item wide bins \(\rightarrow\) low resolution plot, bin
      estimation from more points
    \item narrow bins \(\rightarrow\) high resolution plot, bin
      estimation from less points
    \end{itemize}
  \end{itemize}
  DALE enables wide bins without creating out of distribution instances
\end{frame}


\begin{frame}
  \frametitle{DALE uses on-distribution samples - Experiments}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \[ f(x_1, x_2, x_3) = x_1x_2 + x_1x_3 \: \textcolor{red}{ \pm \: g(x)}\]
      \[ x_1 \in [0,10], x_2 \sim x_1 + \epsilon, x_3 \sim \mathcal{N}(0, \sigma^2) \]
      \[ f_{\mathtt{ALE}}(x_1) = \frac{x_1^2}{2} \]
      \begin{itemize}
      \item point effects affected by \((x_1x_3)\) (\(\sigma\) is large)
      \item bin estimation is noisy (samples are few)
      \end{itemize}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics[width=\textwidth]{./figures/f_plot.pdf}
      \end{center}
    \end{column}
  \end{columns}
  Intuition: we need wider bins (more samples per bin)
\end{frame}


\begin{frame}
  \frametitle{DALE vs ALE - 40 Bins}
  \begin{figure}
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_40_bins.pdf}
    \includegraphics<2>[width=.49\textwidth]{./figures/dale_40_bins.pdf}
    \includegraphics<2>[width=.49\textwidth]{./figures/ale_40_bins.pdf}
  \end{figure}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE vs ALE - 20 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_20_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_20_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_20_bins.pdf}
  \end{figure}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{DALE vs ALE - 10 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_10_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_10_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_10_bins.pdf}
  \end{figure}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: starts being OOD, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE vs ALE - 5 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_5_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_5_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_5_bins.pdf}
  \end{figure}
  \begin{itemize}
  \item DALE: on-distribution, robust bin effect \(\rightarrow\) \textcolor{green}{good estimation}
  \item ALE: completely OOD, robust bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE vs ALE - 3 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_3_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_3_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_3_bins.pdf}
  \end{figure}
  \begin{itemize}
  \item DALE: on-distribution, robust bin effect \(\rightarrow\) \textcolor{green}{good estimation}
  \item ALE: completely OOD, robust bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Real Dataset Experiments - Efficiency}
  \begin{itemize}
  \item Bike-sharing dataset
  \item \(y \rightarrow\) daily bike rentals
  \item \(\Vx:\) 10 features, most of them characteristics of the weather
  \end{itemize}

  \begin{table} \tiny
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
      \multicolumn{12}{c}{Efficiency on Bike-Sharing Dataset (Execution Times in seconds)} \\
      \hline\hline
      & \multicolumn{11}{|c}{Number of Features} \\
      \hline
      & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\
      \hline
      \( \mathtt{DALE} \) & 1.17 & \textbf{1.19} & \textbf{1.22} & \textbf{1.24} & \textbf{1.27} & \textbf{1.30} & \textbf{1.36} & \textbf{1.32} & \textbf{1.33} & \textbf{1.37} & \textbf{1.39} \\
      \hline
      \( \mathtt{ALE} \) & \textbf{0.85} & 1.78 & 2.69 & 3.66 & 4.64 & 5.64 & 6.85 & 7.73 & 8.86 & 9.9 & 10.9 \\
      \hline
    \end{tabular}
  \end{table}
  DALE requires almost same time for all features
\end{frame}


\begin{frame}
  \frametitle{Real Dataset Experiments - Accuracy}
  \begin{itemize}
  \item Difficult to compare in real world datasets
  \item We do not know the ground-truth effect
  \item In most features, DALE and ALE agree.
  \item Only \(X_{\mathtt{hour}}\) is an interesting feature
  \end{itemize}

  \begin{figure}[h]
    \centering
    \resizebox{.3\columnwidth}{!}{\input{figures/bike-dataset-dale-comparison.tex}}
    \resizebox{.3\columnwidth}{!}{\input{figures/bike-dataset-ale-comparison.tex}}
    \caption{(Left) DALE (Left) and ALE (Right) plots for
      \(K = \{25, 50, 100\}\)}
  \end{figure}

\end{frame}

\begin{frame}
  \frametitle{What's next?}
  \begin{itemize}
  \item Could we automatically decide the optimal bin sizes?
    \begin{itemize}
    \item Sometimes narrow bins are ok
    \item Sometimes wide bins are needed
    \end{itemize}
  \item What about variable size bins?
  \item Model the uncertainty of the estimation?
  \end{itemize}

  DALE can be a driver for future work
\end{frame}


\begin{frame}[plain,c]
  \Large Thank you!
\end{frame}

%% \begin{frame}[plain,c]
%%   \Large Συμπληρωματικές διαφάνειες
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Τι είναι μια καλή εξήγηση;}
%%   \begin{itemize}
%%   \item Συχνά δε θέλουμε όλη την πληροφορία αλλά επιλεγμένες σημαντικές
%%     πληροφορίες
%%     \begin{itemize}
%%     \item \orange{Οι εξηγήσεις πρέπει να είναι σύντομες}
%%     \item \orange{Εξαίρεση: Ερμηνεία για νομικούς σκοπούς}
%%     \end{itemize}
%%   \item Άλλες φορές θέλουμε αντιπαραδείγματα
%%     \begin{itemize}
%%     \item \orange{τι έπρεπε να έχω κάνει ώστε να εγκριθεί το δάνειό μου;}
%%     \item \orange{Ποια θα ήταν η απόφαση του μοντέλου αν άλλαζα την τιμή ενός
%%       συγκεκριμένου χαρ/κού;}
%%     \end{itemize}
%%   \item Οι εξηγήσεις εξαρτώνται από την εφαρμογή και τις γνώσεις του παραλήπτη
%%   \item Οι εξηγήσεις συχνά εστιάζουν σε ασυνήθιστες τιμές/χαρακτηριστικά
%%     \begin{itemize}
%%     \item $\text{abnormal} \Rightarrow \text{interesting}$
%%     \end{itemize}
%%   \item Αν δεν υπάρχουν ασυνήθιστες τιμές, τότε οι εξηγήσεις πρέπει να είναι
%%     γενικές και να έχουν υψηλή πιθανότητα (δηλ. να εφαρμόζονται σε μεγάλο
%%     ποσοστό δειγμάτων)
%%   \end{itemize}
%% \end{frame}


%% \begin{frame}
%%   \frametitle{Ταξινόμηση μεθόδων ερμηνείας (απλούστερη)}
%%   \begin{figure}
%%     \includegraphics[width=.6\textwidth]{taxonomy}
%%     \caption{\footnotesize Singh et al, Explainable Deep Learning Models in Medical Image
%%       Analysis, Journal of Imaging 6(6):52, 2020}
%%   \end{figure}
%%   %% \begin{itemize}
%%   %% \item Εγγενής (intrisic) ερμηνευσιμότητα ή Post-hoc ερμηνεία;
%%   %% \item Τύπος ερμηνείας;
%%   %%   \begin{itemize}
%%   %%     \item Περιγραφικά στατιστικά για κάθε χαρ/κό, διάγραμμα επίδρασης
%%   %%       χαρ/κών, βάρη μοντέλου, δείγμα εισόδου (counterfactual), κλπ
%%   %%   \end{itemize}
%%   %% \item Γενική μέθοδος ή για συγκεκριμένο μοντέλο;
%%   %% \item Τοπική (local) ή ολική (global);
%%   %% \end{itemize}
%% \end{frame}

\end{document}
