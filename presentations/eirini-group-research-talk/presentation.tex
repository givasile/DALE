\documentclass{beamer}
\setbeamersize{text margin left=3mm,text margin right=3mm}


\usepackage[backend=bibtex, natbib=true, style=authoryear]{biblatex}% \usepackage[style=authoryear]{natbib}
% \bibliographystyle{plain}
\addbibresource{bibliography}

\usepackage{amsmath, bm, amssymb}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
\usepackage{multirow}

\usepackage{tikz}
\usetikzlibrary{matrix,positioning,arrows.meta,arrows,fit,backgrounds,decorations.pathreplacing}

\tikzset{
  mymat/.style={ matrix of math nodes, text height=2.5ex, text
    depth=0.75ex, text width=6.00ex, align=center, column
    sep=-\pgflinewidth, nodes={minimum height=5.0ex}
  },
  mymats/.style={ mymat, nodes={draw,fill=#1}
  },
  mymat2/.style={
    matrix of math nodes, text height=1.0ex, text depth=0.0ex, minimum
    width=5ex, % text
    width=7.00ex, align=center, column sep=-\pgflinewidth
  },
}

\usetikzlibrary{shapes.geometric, arrows, backgrounds, scopes}
\usepackage{pgfplots} \pgfplotsset{width=6.75cm, compat=newest}
\usepackage[utf8]{inputenc} \DeclareUnicodeCharacter{2212}{âˆ’}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}


\newcommand{\obf}[1]{{\color{orange} \textbf{#1}}}
\newcommand{\orange}[1]{{\color{orange} #1}}
\newcommand{\rbf}[1]{{\color{red} \textbf{#1}}}
\newcommand{\red}[1]{{\color{red} #1}}

% Frequently used vectors
\newcommand{\Vu}{\mathbf{u}}
\newcommand{\Vh}{\mathbf{h}}
\newcommand{\Vv}{\mathbf{v}}
\newcommand{\Vw}{\mathbf{w}}
\newcommand{\Vb}{\mathbf{b}}
\newcommand{\Vx}{\mathbf{x}}
\newcommand{\Vy}{\mathbf{y}}
\newcommand{\Vz}{\mathbf{z}}
\newcommand{\Vf}{\mathbf{f}}
\newcommand{\VC}{\mathbf{C}}
\newcommand{\VD}{\mathbf{D}}
\newcommand{\VX}{\mathbf{X}}
\newcommand{\VS}{\mathbf{S}}
\newcommand{\VW}{\mathbf{W}}
\newcommand{\VV}{\mathbf{V}}
\newcommand{\VU}{\mathbf{U}}
\newcommand{\Vth}{\bm{\theta}}
\newcommand{\pmodel}{p_{\text{model}}}
\newcommand{\pdata}{\hat{p}_{\text{data}}}



\usepackage{tikzsymbols}
\usetheme{Boadilla}
\usecolortheme{seahorse}
\newcommand{\thetab}{\boldsymbol{\theta}}
\newcommand{\xb}{\boldsymbol{x}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title[Research Group]{Presentation at Research Group}
\subtitle{DALE: Differential Accumulated Local Effects for efficient and accurate global explanations}
\author[Gkolemis, Vasilis] % (optional)
{Vasilis Gkolemis\inst{!,*}}

\institute[ATH-HUA]{
  \inst{!} ATHENA Research and Innovation Center
  \and %
  \inst{*} Harokopio University of Athens
}

\date{March 2023}

% Use a simple TikZ graphic to show where the logo is positioned
% \logo{\begin{tikzpicture}
% \filldraw[color=red!50, fill=red!25, very thick](0,0) circle (0.5);
% \node[draw,color=white] at (0,0) {LOGO HERE};
% \end{tikzpicture}}

%End of title page configuration block
%------------------------------------------------------------
%The next block of commands puts the table of contents at the
%beginning of each section and highlights the current section:

% \AtBeginSection[]
% {
%   \begin{frame}
%     \frametitle{Program}
%     \tableofcontents[currentsection]
%   \end{frame}
% }

% ------------------------------------------------------------
\begin{document}
\frame{\titlepage}
%---------------------------------------------------------


\begin{frame}
  \frametitle{Who we are}
  \begin{itemize}
  \item \href{https://givasile.github.io/}{Vasilis Gkolemis}:
    \begin{itemize}
      \item Research Assistant at ATHENA Research Center (\href{https://www.athenarc.gr/en/home}{ATHENA RC})
      \item First-year PhD at Harokopio University of Athens
        (\href{https://dit.hua.gr/index.php/en/}{HUA})
      \item Main focus: Explainability under uncertainty
      \end{itemize}

    \item Supervisors:
      \begin{itemize}
      \item \href{https://diou.github.io/}{Christos Diou} (HUA) \(\rightarrow\) Generalization, Few(Zero)-shot learning
      \item \href{https://aiml-research.github.io/}{Eirini Ntoutsi} (UniBw-M) \(\rightarrow\) Explainability, Fairness
        \item \href{https://scholar.google.gr/citations?user=WJOLNAYAAAAJ&hl=en}{Theodore Dalamagas} (ATHENA) \(\rightarrow\) Databases, data semantics
      \end{itemize}

    \item Paper I will present
      \begin{itemize}
      \item \href{https://givasile.github.io/assets/pdf/gkolemis22_dale.pdf}{DALE: Differential Accumulated Local Effects for efficient and accurate global explanations}
      \item Accepted at \href{https://www.acml-conf.org/2022/}{Asian Conference Machine Learning (ACML) 2022}
      \end{itemize}
    \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Questions}
  \begin{itemize}
  \item Why did the model make a specific decision? \textcolor{red}{local XAI}
  \item What could we change so that the model will make a different decision? \textcolor{red}{counterfactual}
  \item Can we summarize the model's behavior? \textcolor{red}{global XAI}
  \item If models are knowledge extractors, what have they learned? \textcolor{red}{global XAI}
  \end{itemize}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  Feature Effect: global, model-agnostic, outputs a \(1D\) plot
\end{frame}


\begin{frame}
  \frametitle{Example}
  \begin{onlyenv}<1>
    Consider the following mapping $x \rightarrow y$
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_1_reality.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<2>
    Process unknown \(\rightarrow\) we only have samples
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_2_sampling.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<3>
    Our goal is to model the process using the available samples (regression)
    \vspace{1cm}\\
  \end{onlyenv}
  \begin{onlyenv}<4>
    Linear model \(\rightarrow\) Underfiting!
    \begin{equation*}
      y = w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_3_linear.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<5>
    2$^{nd}$ degree polynomial \(\rightarrow\) Decent Fit!
    \begin{equation*}
      y = w_2\cdot x^2 + w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_4_quadratic.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<6>
    3$^{rd}$ degree polynomial \(\rightarrow\) Good Fit!
    \begin{equation*}
      y = w_3\cdot x^3 + w_2\cdot x^2 + w_1\cdot x + w_0
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_5_3d.pgf}
      }
    \end{center}
  \end{onlyenv}
  \begin{onlyenv}<7>
    9$^{th}$ degree polynomial \(\rightarrow\) Overfitting!
    \begin{equation*}
      y = \sum_{i=0}^{9}w_i\cdot x^{i}
    \end{equation*}
    \begin{center}
      \scalebox{0.5}{
        \input{figures/ovf_6_9d.pgf}
      }
    \end{center}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{Problem diagnosis}

  \begin{itemize}
  \item Model behavior is \obf{explained} by the shape of the function
  \item Overfitting, Underfitting are easily diagnosed
  \item If the input has multiple dimensions $D$?
    \begin{itemize}
    \item We often have tens or hundreds of features
    \item Images and signals: Several thousands of input dimensions
    \end{itemize}
  \item Example: \href{https://link.springer.com/chapter/10.1007/978-3-319-58838-4_27}{Risk Factors for Cervical Cancer Dataset}
    \begin{itemize}
    \item input: 15 features (smoker, years of hormonal contraceptives, age)
    \item output: predict whether a woman will get cervical cancer
    \end{itemize}

  \end{itemize}


\end{frame}




\begin{frame}
  \frametitle{Feature Effect}
  \(y = f(x_s) \rightarrow\) plot showing the effect of \(x_s\) on the output \(y\)
  \vspace{2mm}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{./figures/pdp-cervical-1.jpeg}
    \caption{Image taken from Interpretable ML book~\citep{molnar2022}}
  \end{figure}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  Feature Effect is simple and intuitive.
\end{frame}

\begin{frame}
  \frametitle{Feature Effect Methods}
  \begin{itemize}
  \item \(x_s \rightarrow \) feature of interest, \(\bm{x_c} \rightarrow\) other features
  \item Isolating the effect of \(x_s\) is a difficult task:
    \begin{itemize}
    \item features are correlated
      \item \(f\) has learned complex interactions
      \end{itemize}

    \item Three well-known methods:
      \begin{itemize}
      \item Partial Dependence Plots (PDP)
      \item M-Plots
      \item Accumulated Local Effects (ALE)
      \end{itemize}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Partial Dependence Plots (PDP)}
  \begin{onlyenv}<1>
    \begin{itemize}
    \item Proposed by J. Friedman on 2001\footnote{J. Friedman. ``Greedy
    function approximation: A gradient boosting machine.'' Annals of statistics
    (2001): 1189-1232} and is the marginal \obf{effect} of a feature to the
      model output:
      \begin{equation*}
        f_s(x_s) = \mathbb{E}_{\bm{x_c}}\left[f(x_s, \bm{x_c})\right] =
        \int f(x_s, \bm{x_c}) p(\bm{x_c})d\bm{x_c}
      \end{equation*}
      where:

      \begin{itemize}
      \item $x_s$ is the feature whose effect we wish to compute
      \item $\bm{x_c}$ are the rest of the features
      \end{itemize}

    \item Approximation:
      \begin{equation*}
        \hat{f}_s(x_s) = \frac{1}{n}\sum\limits_{i=1}^{n}f(x_s, \Vx_c^{(i)})
      \end{equation*}
    \end{itemize}
  \end{onlyenv}
\end{frame}


\begin{frame}
  \frametitle{Issues with PDPs}
  \begin{onlyenv}<1>
    \begin{figure}
      \includegraphics[width=.6\textwidth]{./figures/aleplot-motivation1-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
  \begin{onlyenv}<2>
    \begin{itemize}
    \item Correlated features
      \begin{itemize}
      \item To compute the effect of \(x_{\mathtt{age}} = 20\) on the
        output (cancer probability) it will integrate over all
        \(x_{\mathtt{years\_contraceptives}}\) values, e.g., \([0, 50]\)
      \item \(f\) can have weird behavior when
        \(x_{\mathtt{age}} = 20, x_{\mathtt{years\_contraceptives}}=
        20\) (out of distribution)
      \item As a result, we have a wrong estimation of the feature effect
      \end{itemize}
      \end{itemize}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{MPlots}

  \begin{onlyenv}<1>
    \begin{itemize}
    \item We use the value of $x_s$ as a condition, so we integrate over \(\Vx_c|x_s\)

      \[f(x_s) = \mathbb{E}_{\Vx_c|x_s}[f(x_s, \Vx_c)] = \int f(x_s, \Vx_c) p(\bm{x_c}|x_s) d\bm{x_c} \]
      where:

      \begin{itemize}
      \item $x_s$ is the feature whose effect we wish to compute
      \item $\bm{x_c}$ the rest of the features
      \end{itemize}

    \item Approximation:
      \begin{equation*}
        f_s(x_s) = \frac{1}{n}\sum\limits_{i \text{:} x_s^{(i)} \approx x_s} f(x_s, \Vx_c^{(i)})
      \end{equation*}

    \end{itemize}
  \end{onlyenv}
  \begin{onlyenv}<2>
    In the previous example
    \begin{figure}
      \includegraphics[width=.6\textwidth]{./figures/aleplot-motivation2-1}
      \caption{\footnotesize C. Molnar, IML book}
    \end{figure}
  \end{onlyenv}
\end{frame}

\begin{frame}
  \frametitle{Issues with M-Plots}
  \begin{itemize}
  \item Aggregated effect symptom \(\rightarrow\) the calculated
    effects result from the combination of all (correlated) features
  \item Real effect:
    \begin{itemize}
    \item \(x_{\mathtt{age}} = 50 \rightarrow 10\)
    \item  \(x_{\mathtt{years\_contraceptives}} = 20 \rightarrow 10\)
    \item  aggregated effect close to \(20\)
    \end{itemize}
  \item Because \(x_{\mathtt{age}}, x_{\mathtt{years\_contraceptives}}\) are correlated, MPlot may assign:
    \begin{itemize}
    \item \(x_{\mathtt{age}} = 50 \rightarrow 17 \approx \) aggregated effect
    \item  \(x_{\mathtt{years\_contraceptives}} = 20 \rightarrow 17 \approx \) aggregated effect
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Accumulated Local Effects (ALE)\footnote{D. Apley and
    J. Zhu. ``Visualizing the effects of predictor variables in black box
    supervised learning models.'' Journal of the Royal Statistical Society:
    Series B (Statistical Methodology) 82.4 (2020): 1059-1086.}}

  \begin{itemize}
  \item Resolves problems that result from the feature correlation by computing
    differences over a (small) window
  \end{itemize}

  \[f(x_s) = \int_{x_{min}}^{x_s} \mathbb{E}_{\underbrace{\Vx_c|z}_{realistic}} [ \underbrace{\frac{\partial f}{\partial x_s}(z, \Vx_c)}_{isolates} ] \partial z\]

\end{frame}






\begin{frame}
  \frametitle{ALE approximation}
  ALE definition: \( f(x_s) = \int_{x_{s, min}}^{x_s}\mathbb{E}_{\bm{x_c}|z}[ \frac{\partial f}{\partial x_s}(z, \bm{x_c})] \partial z \)
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  ALE approximation: \(f(x_s) = \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[f(z_k, \bm{x^i_c}) - f(z_{k-1}, \bm{x^i_c})]}_{\text{point effect}}}_{\text{bin effect}} \)

  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{./figures/ale_bins_iml.png}
    \caption{Image taken from Interpretable ML book~\citep{molnar2022}}
  \end{figure}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  Bin splitting (parameter \(K\)) is crucial!
\end{frame}


\begin{frame}
  \frametitle{ALE approximation - weaknesses}

    \[f(x_s) = \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[f(z_k, \bm{x^i_c}) - f(z_{k-1}, \bm{x^i_c})]}_{\text{point effect}}}_{\text{bin effect}} \]

    \begin{itemize}
      \item Point Effect \(\Rightarrow\) evaluation \alert{at bin limits}
    \begin{itemize}
    \item 2 evaluations of \(f\) per point \( \rightarrow \) slow
    \item change bin limits, pay again \(2*N\) evaluations of \(f\) \( \rightarrow\) restrictive
    \item broad bins may create out of distribution (OOD) samples \( \rightarrow\) not-robust in wide bins
    \end{itemize}
    \end{itemize}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  ALE approximation has some weaknesses
\end{frame}


\begin{frame}
  \frametitle{Recap!}
  \begin{itemize}
  \item PDP \(\rightarrow\) problems with correlated features \(\rightarrow\) Unrealistic instances
  \item MPlot \(\rightarrow\) problems with correlated features \(\rightarrow\) Aggregated effects
  \item ALE \(\rightarrow\) resolves both issues! But:

  \item ALE approximation (estimation of ALE from the training set)
    \begin{itemize}
    \item slow when there are many features
      \item unrealistic instances when bins become bigger
      \end{itemize}
      \item Differential ALE (DALE)!
  \end{itemize}
  \end{frame}


\begin{frame}
  \frametitle{Our proposal: Differential ALE}
    \[f(x_s) = \Delta x \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial f}{\partial x_s}(x_s^i, \bm{x^i_c})]}_{\text{\alert{point effect}}}}_{\text{bin effect}} \]

    \begin{itemize}
      \item Point Effect \(\Rightarrow\) evaluation \alert{on instances}
    \begin{itemize}
    \item Fast \( \rightarrow \) use of auto-differentiation, all derivatives in a single pass
    \item Versatile \( \rightarrow\) point effects computed once, change bins without cost
    \item Secure \( \rightarrow\) does not create artificial instances
    \end{itemize}
    \end{itemize}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  For \alert{differentiable} models, DALE resolves ALE weaknesses
\end{frame}



% chapter 1
\section{DALE vs ALE}

\subsection{Dale is faster and versatile}

\begin{frame}
  \frametitle{DALE is faster and versatile - theory}
    \[f(x_s) = \Delta x \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|} \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial f}{\partial x_s}(x_s^i, \bm{x^i_c})]}_{\text{point effect}}}_{\text{bin effect}} \]

  \begin{itemize}
  \item Faster
    \begin{itemize}
      \item gradients wrt all features \(\nabla_{\bm{x}} f(\bm{x^i})\) in a single pass
      \item auto-differentiation must be available (deep learning)
    \end{itemize}
  \item Versatile
    \begin{itemize}
    \item Change bin limits, with near zero computational cost
    \end{itemize}

  \end{itemize}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  DALE is faster and allows redefining bin-limits
\end{frame}

\begin{frame}
  \frametitle{DALE is faster and versatile - Experiments}
  \begin{figure}[h]
  \centering
  \resizebox{.4\columnwidth}{!}{\input{./../../ACML-paper/images/case-1-plot-1.tex}}
  \resizebox{.43\columnwidth}{!}{\input{./../../ACML-paper/images/case-1-plot-2.tex}}
  \caption[Case-1-fig-1]{Light setup; small dataset \((N=10^2\) instances), light \(f\). Heavy setup; big dataset (\(N=10^5\) instances), heavy \(f\)}
  \label{fig:case-1-plots-1}
\end{figure}

  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  DALE considerably accelerates the estimation
\end{frame}


\subsection{DALE is more Accurate}

\begin{frame}
  \frametitle{DALE uses on-distribution samples - Theory}
  \[f(x_s) = \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|}
      \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial
          f}{\partial x_s}(x_s^i, \bm{x^i_c})]}_{\text{point
          effect}}}_{\text{bin effect}} \]

  \begin{itemize}
  \item point effect \alert{independent} of bin limits
    \begin{itemize}
    \item \(\frac{\partial f}{\partial x_s}(x_s^i, \bm{x^i_c})\)
      computed on real instances \(\bm{x}^i = (x_s^i, \bm{x_c^i})\)
    \end{itemize}
  \item bin limits affect only the \alert{resolution} of the plot
    \begin{itemize}
    \item wide bins \(\rightarrow\) low resolution plot, bin
      estimation from more points
    \item narrow bins \(\rightarrow\) high resolution plot, bin
      estimation from less points
    \end{itemize}
  \end{itemize}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  DALE enables wide bins without creating out of distribution instances
\end{frame}


\begin{frame}
  \frametitle{DALE uses on-distribution samples - Experiments}
\begin{columns}
\begin{column}{0.5\textwidth}
  \[ f(x_1, x_2, x_3) = x_1x_2 + x_1x_3 \: \textcolor{red}{ \pm \: g(x)}\]
  \[ x_1 \in [0,10], x_2 \sim x_1 + \epsilon, x_3 \sim \mathcal{N}(0, \sigma^2) \]
  \[ f_{\mathtt{ALE}}(x_1) = \frac{x_1^2}{2} \]
  \begin{itemize}
  \item point effects affected by \((x_1x_3)\) (\(\sigma\) is large)
  \item bin estimation is noisy (samples are few)
  \end{itemize}
\end{column}
\begin{column}{0.5\textwidth}
    \begin{center}
     \includegraphics[width=\textwidth]{./figures/f_plot.pdf}
     \end{center}
\end{column}
\end{columns}
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
Intuition: we need wider bins (more samples per bin)
\end{frame}


\begin{frame}
  \frametitle{DALE vs ALE - 40 Bins}
  \begin{figure}
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_40_bins.pdf}
    \includegraphics<2>[width=.49\textwidth]{./figures/dale_40_bins.pdf}
    \includegraphics<2>[width=.49\textwidth]{./figures/ale_40_bins.pdf}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE vs ALE - 20 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_20_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_20_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_20_bins.pdf}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{DALE vs ALE - 10 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_10_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_10_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_10_bins.pdf}
    \label{}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \item ALE: starts being OOD, noisy bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE vs ALE - 5 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_5_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_5_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_5_bins.pdf}
    \label{}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, robust bin effect \(\rightarrow\) \textcolor{green}{good estimation}
  \item ALE: completely OOD, robust bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{DALE vs ALE - 3 Bins}
  \begin{figure}[ht]
    \centering
    \includegraphics<1>[width=0.6\textwidth]{./figures/bin_splitting_3_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/dale_3_bins.pdf}
    \includegraphics<2>[width=0.49\textwidth]{./figures/ale_3_bins.pdf}
    \label{}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  \begin{itemize}
  \item DALE: on-distribution, robust bin effect \(\rightarrow\) \textcolor{green}{good estimation}
  \item ALE: completely OOD, robust bin effect \(\rightarrow\) \textcolor{red}{poor estimation}
  \end{itemize}
\end{frame}


% \begin{frame}
%   \frametitle{Real Dataset Experiments - Efficiency}
%   \begin{itemize}
%   \item Bike-sharing dataset\citep{BikeSharing}
%   \item \(y \rightarrow\) daily bike rentals
%   \item \(\bm{x}:\) 10 features, most of them characteristics of the weather
%   \end{itemize}

%   \begin{table} \tiny
%   \centering
%   \begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c}
%     \multicolumn{12}{c}{Efficiency on Bike-Sharing Dataset (Execution Times in seconds)} \\
%     \hline\hline
%     & \multicolumn{11}{|c}{Number of Features} \\
%     \hline
%     & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\
%     \hline
%     \( \mathtt{DALE} \) & 1.17 & \textbf{1.19} & \textbf{1.22} & \textbf{1.24} & \textbf{1.27} & \textbf{1.30} & \textbf{1.36} & \textbf{1.32} & \textbf{1.33} & \textbf{1.37} & \textbf{1.39} \\
%     \hline
%     \( \mathtt{ALE} \) & \textbf{0.85} & 1.78 & 2.69 & 3.66 & 4.64 & 5.64 & 6.85 & 7.73 & 8.86 & 9.9 & 10.9 \\
%     \hline
%   \end{tabular}
% \end{table}
% \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
% DALE requires almost same time for all features
% \end{frame}


% \begin{frame}
%   \frametitle{Real Dataset Experiments - Accuracy}
%   \begin{itemize}
%   \item Difficult to compare in real world datasets
%    \item We do not know the ground-truth effect
%    \item In most features, DALE and ALE agree.
%    \item Only \(X_{\mathtt{hour}}\) is an interesting feature
%   \end{itemize}

%   \begin{figure}[h]
%   \centering
%     \resizebox{.3\columnwidth}{!}{\input{./../../ACML-paper/images/bike-dataset-dale-comparison.tex}}
%     \resizebox{.3\columnwidth}{!}{\input{./../../ACML-paper/images/bike-dataset-ale-comparison.tex}}
%     \caption{(Left) DALE (Left) and ALE (Right) plots for
%       \(K = \{25, 50, 100\}\)}
% \end{figure}

% \end{frame}



\begin{frame}
  \frametitle{Future Ideas (1)}
  PDPs use ICE plots, for exhibiting heterogeneity
  \begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/pdp}
    \caption{PDP plot, taken from \href{https://arxiv.org/abs/1309.6392}{Goldstein et. al}}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  Interpretation?
  Maybe $y \perp\!\!\!\!\perp x_2$
\end{frame}

\begin{frame}
  \frametitle{Future Ideas (2)}
  PDPs use ICE plots, for exhibiting heterogeneity
  \begin{figure}
    \centering
    \includegraphics[width=0.4\textwidth]{figures/pdp_ice}
    \caption{PDP-ICE plot, taken from \href{https://arxiv.org/abs/1309.6392}{Goldstein et. al}}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  Interpretation now? Maybe $y \approx \pm 6 x_2 $ depending on a condition
\end{frame}

\begin{frame}
  \frametitle{Future Ideas (3)}
  \begin{itemize}
  \item Could ALE plots do the same?
  \item Variance inside each bin?
  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/rhale}
    \caption{(Left) PDP-ICE (Right) ALE with heterogeneity}
  \end{figure}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Future Ideas (4) - Regional Effect plots}
  \begin{itemize}
    \item Heterogeneity $\rightarrow$ subspaces with homogeneous effects
  \end{itemize}

  \begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/repid}
    \caption{REPID: Regional Effect plots, taken from \href{https://arxiv.org/abs/2202.07254}{Herbinger et. al}}
  \end{figure}
  \noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
  Same idea on ALE?
\end{frame}




\begin{frame}
  \frametitle{Thank you}
  \begin{itemize}
  \item Questions?
  \end{itemize}

\end{frame}



\begin{frame}[allowframebreaks]
  \frametitle{References}
  \printbibliography
\end{frame}


\end{document}