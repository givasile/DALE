\documentclass[final]{beamer}
%% Possible paper sizes: a0, a0b, a1, a2, a3, a4.
%% Possible orientations: portrait, landscape
%% Font sizes can be changed using the scale option.
\usepackage[size=a0,orientation=portrait,scale=1.3]{beamerposter}

\usetheme{gemini}
\usecolortheme{seagull}
\useinnertheme{rectangles}

% ====================
% Packages
% ====================

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{bm, amsmath}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
% \usepackage[most]{tcolorox}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}

\newcommand{\xb}{\mathbf{x}}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.03\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Logo (optional)
% ====================

% LaTeX logo taken from https://commons.wikimedia.org/wiki/File:LaTeX_logo.svg
% use this to include logos on the left and/or right side of the header:
\logoright{\includegraphics[height=5cm]{logos/athena-278x300.png}}
\logoleft{\includegraphics[height=5cm]{logos/dit-logo.png}}

% ====================
% Footer (optional)
% ====================

\footercontent{
	ACML 2022, Hyderabad, India \hfill
	\insertdate \hfill
	\href{mailto:vgkolemis@athenarc.gr}{\texttt{vgkolemis@athenarc.gr}}
}
% (can be left out to remove footer)

% ====================
% My own customization
% - BibLaTeX
% - Boxes with tcolorbox
% - User-defined commands
% ====================
\input{custom-defs.tex}

%% Reference Sources
\addbibresource{refs.bib}
\renewcommand{\pgfuseimage}[1]{\includegraphics[scale=2.0]{#1}}

\title{DALE: Differential Accumulated Local Effects for efficient and accurate global explanations}

\author{Vasilis Gkolemis \inst{1, 2} \and Theodore Dalamagas \inst{1} \and Christos Diou \inst{2}}

\institute[shortinst]{\inst{1} ATHENA Research Center \samelineand \inst{2} Harokopio University of Athens}

\date{November 12-14, 2022}

\begin{document}
	
\begin{frame}[t]
	\begin{columns}[t] \separatorcolumn
		\begin{column}{\colwidth}
      \begin{alertblock}{TL;DR} \Large{\textbf{DALE} is a better
          approximation to ALE}, the SotA feature effect method. By better, we
        mean faster and more accurate.

        \large{\textbf{keywords:} eXplainable AI, global,
          model-agnostic, deep learning}
			\end{alertblock}

			\begin{block}{Motivation}{}
        Feature effect (FE) plots are simple
        and intuitive; they isolate the impact of a single feature
        \(x_s\) on the output \(y\). By inspecting a FE plot, a
        non-expert can quickly understand whether a feature has
        positive/negative impact (and to what extent) on the target
        variable.

        This simplicity comes at a cost; isolating the effect of a
        single variable on the output is tricky because normally,
        features are correlated and the black-box function learns
        complex input-output mappings. ALE~(\cite{Apley2020}) is the
        SotA feature effect method because it handles well 
        correlated features. However, ALE estimation, i.e., the
        approximation of ALE from the instances of the training set,
        has some drawbacks; it becomes inefficient in high-dimensional
        datasets and it is vulnerable to creating synthetic
        out-of-distribution instances.

        In this work, we analyze these drawbacks and propose
        Differential ALE (DALE), a novel approximation, that we
        addresses them.
			\end{block}

      %%%%%% DALE vs ALE
      \begin{defbox}{DALE vs ALE}{}
        \begin{tcolorbox}[ams equation*, title=ALE definition]
          f(x_s) = \int_{x_{s, min}}^{x_s}\mathbb{E}_{\bm{x_c}|z}\underbrace{[\frac{\partial f}{\partial x_s}(z,\bm{x_c})]}_{\text{point effect}} \partial z
        \end{tcolorbox}

        ALE defines the effect at \(x_s=z\) as the expected change
        (derivative) on the output over the conditional distribution
        \(\bm{x_c}|z\). The feature effect plot is the accumulation of
        the expected changes.

        \vspace{2mm}

        ALE approximation, i.e., estimating ALE from the training set
        \(\mathcal{D}\), requires partitioning the \(s\)-th axis in
        \(K\) equisized bins. The value of the parameter \(K\) has
        crucial consequences on the final curve. If \(K\) is high
        (narrow bins), we get a high-resolution plot but with limited
        samples per bin (noisy estimation). If \(K\) is low (wide
        bins), we get a low-resolution plot but with more samples per
        bin (robust estimation).

        \vspace{2mm}
        \begin{tcolorbox}[ams equation*, title=ALE approximation]
          f(x_s) = \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|}
            \sum_{i:\mathbf{x}^i \in \mathcal{S}_k} \underbrace{[f(z_k,
              \bm{x^i_c}) - f(z_{k-1}, \bm{x^i_c})]}_{\text{point
                effect}}}_{\text{bin effect}}
        \end{tcolorbox}

        ALE computes the point effects by evaluating \(f\) at the bin
        limits: \([f(z_k, \bm{x^i_c}) - f(z_{k-1},
        \bm{x^i_c})]\). This approach has some drawbacks:

        \begin{enumerate}
        \item requires \(\mathcal{O}(N*D)\) evaluations of \(f\)
        \item recomputes all effects from scratch if changing \(K\)
        \item creates artificial samples, that my become OOD when the bin size is large
        \end{enumerate}

        \vspace{2mm}
        \begin{tcolorbox}[ams equation*, title=DALE approximation] f(x_s) =
          \Delta x \sum_k^{k_x} \underbrace{\frac{1}{|\mathcal{S}_k|}
            \sum_{i:\xb^i \in \mathcal{S}_k} \underbrace{[\frac{\partial
                f}{\partial x_s}(x_s^i, \bm{x^i_c})]}_{\text{\alert{point
                  effect}}}}_{\text{bin effect}}
        \end{tcolorbox}

        For adressing these issues we propose DALE, an alternative
        approximation that computes the point effects by evaluating
        the derivatives \(\frac{\partial f}{\partial x_s}\) on the
        dataset instances.

        \begin{enumerate}
        \item requires \(\mathcal{O}(N)\) evaluations of \(f\)
        \item local effects are decoupled from bin size \(K\)
        \item does not create artificial samples
        \end{enumerate}

  \end{defbox}

\end{column}

\separatorcolumn
\begin{column}{\colwidth}

  % DALE ACCURACY
  \begin{block}{DALE is accurate}
    \begin{tabular}{cl}
      \begin{tabular}{c}
        \includegraphics[width=.45\textwidth]{./../ACML-presentation/figures/bin_splitting_5_bins.pdf}
      \end{tabular} &
      \begin{tabular}{l}
        \parbox{0.49\linewidth}{Consider the following case; (a)
        we have limited samples (b) high variance in some features and (c) the black-box
        function changes abruptply outside of the data manifold. For example, \(
        f(x_1, x_2, x_3) = x_1x_2 + x_1x_3 \: \textcolor{red}{ \pm \: g(x)}\),
        with \(x_1 \in [0,10]\), \(x_2=x_1 + \epsilon\) and \(x_3 \sim
        \mathcal{N}(0, \sigma^2)\). The term \(x_1x_3\) makes estimations from
        limited samples (narrow bins) noisy, see Figure~\ref{fig:acc-1}.
        If we use larger bins (more \(\frac{\text{points}} { \text{bin}} \)),
        DALE leads to a good estimation whereas ALE fails due to OOD samples.}
      \end{tabular}
    \end{tabular}

    \begin{figure}
      \centering
      \includegraphics[width=0.49\textwidth]{./../ACML-presentation/figures/dale_40_bins.pdf}
      \includegraphics[width=0.49\textwidth]{./../ACML-presentation/figures/ale_40_bins.pdf}
      \caption{Narrow bins (\(K=40\)) \(\Rightarrow\) limited
        \(\frac{\text{samples}}{\text{bin}}\) \(\Rightarrow\) both plots are noisy }
      \label{fig:acc-1}
    \end{figure}

    \begin{figure} \centering
      \includegraphics[width=0.49\textwidth]{./../ACML-presentation/figures/dale_5_bins.pdf}
      \includegraphics[width=0.49\textwidth]{./../ACML-presentation/figures/ale_5_bins.pdf}
      \caption{Wide bins (\(K=5\)) \(\Rightarrow\) many
        \(\frac{\text{samples}}{\text{bin}}\) \(\Rightarrow\) DALE is
        accurate, ALE is affected by OOD}
    \end{figure}
  \end{block}

  % DALE EFFICIENCY
  \begin{block}{DALE is fast}
    In a large and high dimensional dataset, ALE needs 10 mins, DALE some seconds!

    We test DALE vs ALE in two setups
    (Figure~\ref{fig:efficiency}). The light and heavy setup differ in
    the size of the dataset (\(N=10^2\) vs \(N=10^5\) instances) and
    the cost of evaluating \(f\) (light vs heavy). In both cases, DALE
    scales much better wrt dimensionality D.
    \begin{figure}[ht] \centering
      \includegraphics[width=0.49\textwidth]{./../ACML-paper/images/case-1-plot-1.pdf}
      \includegraphics[width=0.49\textwidth]{./../ACML-paper/images/case-1-plot-2.pdf}
      \caption[Case-1-fig-1]{Light setup; small dataset
        \((N=10^2\) instances), light \(f\). Heavy setup; big dataset
        (\(N=10^5\) instances), heavy \(f\)}
      \label{fig:efficiency}
    \end{figure}
  \end{block}
\end{column} \separatorcolumn
\end{columns}

\begin{columns}[t]\separatorcolumn
    \begin{column}{1.3\colwidth}
      \large
      \begin{alertblock}{Conclusion} In case you work with a
        differentiable model, as in Deep Learning, use DALE to:
        \begin{itemize}
        \item compute the effect of all features efficiently
        \item test the FE plot for many different bin sizes \(K\), without computational cost
        \item ensure on-distribution estimation, irrespectively of the bin size
      \end{itemize}
    \end{alertblock}
	\end{column} \separatorcolumn
	\begin{column}{0.7\colwidth}
    \begin{block}{References}
      \printbibliography[heading=none]
      \begin{itemize}
      \item \large \href{givasile.github.io}{givasile.github.io}, \href{https://twitter.com/givasile1}{twitter.com/givasile1}
      \end{itemize}
      
	\end{block}
\end{column}
\separatorcolumn
\end{columns}


\end{frame}

\end{document}