@article{Hooker2021,
author={Hooker, G. and Mentch, L. and Zhou, S.},
title={Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance},
journal={Statistics and Computing},
year={2021},
volume={31},
number={6},
doi={10.1007/s11222-021-10057-z},
art_number={82},
note={cited By 0},
document_type={Article},
source={Scopus},
}

@article{Friedman2001,
abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization iti function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitives highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
author = {Friedman, Jerome H.},
doi = {10.1214/aos/1013203451},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman - 2001 - Greedy function approximation A gradient boosting machine.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Boosting,Decision trees,Function estimation,Robust nonparametric regression},
mendeley-groups = {Feature Effect paper},
month = {oct},
number = {5},
pages = {1189--1232},
publisher = {Institute of Mathematical Statistics},
title = {{Greedy function approximation: A gradient boosting machine}},
volume = {29},
year = {2001}
}



@article{Apley2020,
abstract = {In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
annote = {The paper that proposed ALE plots.},
archivePrefix = {arXiv},
arxivId = {1612.08468},
author = {Apley, Daniel W. and Zhu, Jingyu},
doi = {10.1111/rssb.12377},
eprint = {1612.08468},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Apley, Zhu - 2020 - Visualizing the effects of predictor variables in black box supervised learning models.pdf:pdf},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Functional analysis of variance,Marginal plots,Partial dependence plots,Supervised learning,Visualization},
mendeley-groups = {ΧΑΙ,Feature Effect paper},
number = {4},
pages = {1059--1086},
title = {{Visualizing the effects of predictor variables in black box supervised learning models}},
volume = {82},
year = {2020}
}

@article{BikeSharing,
year={2013},
issn={2192-6352},
journal={Progress in Artificial Intelligence},
doi={10.1007/s13748-013-0040-3},
title={Event labeling combining ensemble detectors and background knowledge},
url={[Web Link]},
publisher={Springer Berlin Heidelberg},
keywords={Event labeling; Event detection; Ensemble learning; Background knowledge},
author={Fanaee-T, Hadi and Gama, Joao},
pages={1-15}
}

@article{Hoffman2018,
  title={Metrics for explainable AI: Challenges and prospects},
  author={Hoffman, Robert R and Mueller, Shane T and Klein, Gary and Litman, Jordan},
  journal={arXiv preprint arXiv:1812.04608},
  year={2018}
}



@article{Adadi2018,
abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
author = {Adadi, Amina and Berrada, Mohammed},
doi = {10.1109/ACCESS.2018.2870052},
issn = {21693536},
journal = {IEEE Access},
keywords = {Explainable artificial intelligence,black-box models,interpretable machine learning},
mendeley-groups = {Feature Effect paper},
month = {sep},
pages = {52138--52160},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)}},
volume = {6},
year = {2018}
}

@article{BarredoArrieta2020,
abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
archivePrefix = {arXiv},
arxivId = {1910.10045},
author = {{Barredo Arrieta}, Alejandro and D{\'{i}}az-Rodr{\'{i}}guez, Natalia and {Del Ser}, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
doi = {10.1016/j.inffus.2019.12.012},
eprint = {1910.10045},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Barredo Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concepts, Taxonomies, Opportunities and Challenges toward Resp.pdf:pdf},
issn = {15662535},
journal = {Information Fusion},
keywords = {Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable Artificial Intelligence,Fairness,Interpretability,Machine Learning,Privacy,Responsible Artificial Intelligence,Transparency},
mendeley-groups = {Feature Effect paper},
pages = {82--115},
title = {{Explainable Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI}},
volume = {58},
year = {2020}
}

@article{Molnar2020,
abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
archivePrefix = {arXiv},
arxivId = {2010.09337},
author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
doi = {10.1007/978-3-030-65965-3_28},
eprint = {2010.09337},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Molnar - Unknown - Interpretable Machine Learning-A Brief History, State-of-the-Art and Challenges.pdf:pdf},
isbn = 9783030659646,
issn = 18650937,
journal = {Communications in Computer and Information Science},
keywords = {Explainable artificial intelligence,Interpretable Machine Learning},
mendeley-groups = {Feature Effect paper},
pages = {417--431},
title = {{Interpretable Machine Learning – A Brief History, State-of-the-Art and Challenges}},
volume = 1323,
year = 2020
}


@article{Fisher2019,
abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model f(x) = xT$\beta$ with a fixed coefficient vector $\beta$) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
archivePrefix = {arXiv},
arxivId = {1801.01489},
author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
eprint = {1801.01489},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fisher, Rudin, Dominici - 2019 - All Models are Wrong, but Many are Useful Learning a Variable's Importance by Studying an Entire Class.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Conditional variable importance,Interpretable models,Permutation importance,Rashomon,Transparency,U-statistics},
mendeley-groups = {Feature Effect paper},
pages = {1--81},
pmid = {34335110},
title = {{All models are wrong, but many are useful: Learning a variable's importance by studying an entire class of prediction models simultaneously}},
url = {http://jmlr.org/papers/v20/18-760.html.},
volume = {20},
year = {2019}
}

@article{Greenwell2018,
abstract = {In the era of "big data", it is becoming more of a challenge to not only build state-of-the-art predictive models, but also gain an understanding of what's really going on in the data. For example, it is often of interest to know which, if any, of the predictors in a fitted model are relatively influential on the predicted outcome. Some modern algorithms---like random forests and gradient boosted decision trees---have a natural way of quantifying the importance or relative influence of each feature. Other algorithms---like naive Bayes classifiers and support vector machines---are not capable of doing so and model-free approaches are generally used to measure each predictor's importance. In this paper, we propose a standardized, model-based approach to measuring predictor importance across the growing spectrum of supervised learning algorithms. Our proposed method is illustrated through both simulated and real data examples. The R code to reproduce all of the figures in this paper is available in the supplementary materials.},
archivePrefix = {arXiv},
arxivId = {1805.04755},
author = {Greenwell, Brandon M. and Boehmke, Bradley C. and McCarthy, Andrew J.},
eprint = {1805.04755},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greenwell, Boehmke, McCarthy - 2018 - A Simple and Effective Model-Based Variable Importance Measure.pdf:pdf},
keywords = {Interaction effect,PDP,Partial de-pendence plot,Partial dependence function,Relative influence},
mendeley-groups = {Feature Effect paper},
month = {may},
title = {{A Simple and Effective Model-Based Variable Importance Measure}},
url = {http://arxiv.org/abs/1805.04755},
year = {2018}
}

@inproceedings{Molnar2021,
abstract = {Post-hoc model-agnostic interpretation methods such as partial dependence plots can be employed to interpret complex machine learning models. While these interpretation methods can be applied regardless of model complexity, they can produce misleading and verbose results if the model is too complex, especially w.r.t. feature interactions. To quantify the complexity of arbitrary machine learning models, we propose model-agnostic complexity measures based on functional decomposition: number of features used, interaction strength and main effect complexity. We show that post-hoc interpretation of models that minimize the three measures is more reliable and compact. Furthermore, we demonstrate the application of these measures in a multi-objective optimization approach which simultaneously minimizes loss and complexity.},
archivePrefix = {arXiv},
arxivId = {1904.03867},
author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
booktitle = {Communications in Computer and Information Science},
doi = {10.1007/978-3-030-43823-4_17},
eprint = {1904.03867},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Molnar, Casalicchio, Bischl - Unknown - Quantifying Model Complexity via Functional Decomposition for Better Post-Hoc Interpretability.pdf:pdf},
isbn = {9783030438227},
issn = {18650937},
keywords = {Accumulated Local Effects,Explainable AI,Interpretable machine learning,Model complexity,Multi-objective optimization},
mendeley-groups = {Feature Effect paper},
pages = {193--204},
title = {{Quantifying model complexity via functional decomposition for better post-hoc interpretability}},
volume = {1167 CCIS},
year = {2020}
}

@inproceedings{Kim2016,
abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the GIST of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.},
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Khanna, Koyejo - 2016 - Examples are not enough, learn to criticize! Criticism for Interpretability.pdf:pdf},
issn = {10495258},
keywords = {global},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {global},
pages = {2288--2296},
title = {{Examples are not enough, learn to criticize! Criticism for interpretability}},
volume = {29},
year = {2016}
}

@article{Ribeiro2016,
abstract = {Despite widespread adoption in NLP, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust in a model. Trust is fundamental if one plans to take action based on a prediction, or when choosing whether or not to deploy a new model. In this work, we describe LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner. We further present a method to explain models by presenting representative individual predictions and their explanations in a non-redundant manner. We propose a demonstration of these ideas on different NLP tasks such as document classification, politeness detection, and sentiment analysis, with classifiers like neural networks and SVMs. The user interactions include explanations of free-form text, challenging users to identify the better classifier from a pair, and perform basic feature engineering to improve the classifiers.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
doi = {10.18653/v1/n16-3020},
eprint = {1602.04938},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ribeiro, Singh, Guestrin - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf:pdf},
journal = {NAACL-HLT 2016 - 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Demonstrations Session},
keywords = {local},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {local},
month = {feb},
pages = {97--101},
publisher = {Association for Computational Linguistics (ACL)},
title = {{"Why Should I Trust You?" Explaining the Predictions of Any Classifier}},
url = {https://arxiv.org/abs/1602.04938v3},
year = {2016}
}

@article{Ribeiro2018,
abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
issn = {2374-3468},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {interpretability,local},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {local},
month = {apr},
number = {1},
title = {{Anchors: High-Precision Model-Agnostic Explanations}},
url = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
volume = {32},
year = {2018}
}

@article{Wachter2017,
abstract = {There has been much discussion of the “right to explanation” in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the ‘black box' of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Data controllers have an interest to not disclose information about their algorithms that contains trade secrets, violates the rights and freedoms of others (e.g. privacy), or allows data subjects to game or manipulate decision-making. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR, and the extent to which they hinge on opening the ‘black box'. We suggest data controllers should offer a particular type of explanation, ‘unconditional counterfactual explanations', to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the “closest possible world.” As multiple variables or sets of variables can lead to one or more desirable outcomes, multiple counterfactual explanations can be provided, corresponding to different choices of nearby possible worlds for which the counterfactual holds. Counterfactuals describe a dependency on the external facts that lead to that decision without the need to convey the internal state or logic of an algorithm. As a result, counterfactuals serve as a minimal solution that bypasses the current technical limitations of interpretability, while striking a balance between transparency and the rights and freedoms of others (e.g. privacy, trade secrets).},
archivePrefix = {arXiv},
arxivId = {1711.00399},
author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
doi = {10.2139/ssrn.3063289},
eprint = {1711.00399},
issn = {1556-5068},
journal = {SSRN Electronic Journal},
keywords = {local},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {local},
month = {nov},
publisher = {Elsevier BV},
title = {{Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR}},
url = {https://arxiv.org/abs/1711.00399v3},
year = {2017}
}

@article{Lundberg2017,
abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
archivePrefix = {arXiv},
arxivId = {1705.07874},
author = {Lundberg, Scott M. and Lee, Su In},
eprint = {1705.07874},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lundberg, Allen, Lee - Unknown - A Unified Approach to Interpreting Model Predictions.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {local},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {local},
pages = {4766--4775},
title = {{A unified approach to interpreting model predictions}},
url = {https://github.com/slundberg/shap},
volume = {2017-December},
year = {2017}
}

@article{Selvaraju2016,
abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
archivePrefix = {arXiv},
arxivId = {1610.02391v4},
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
doi = {10.1007/s11263-019-01228-7},
eprint = {1610.02391v4},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Selvaraju et al. - 2016 - Grad-CAM Visual Explanations from Deep Networks via Gradient-based Localization.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {CNN,Explanations,Grad-CAM,Interpretability,Transparency,Visual explanations,Visualizations},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {CNN},
month = {oct},
number = {2},
pages = {336--359},
publisher = {Springer},
title = {{Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization}},
url = {http://arxiv.org/abs/1610.02391 http://dx.doi.org/10.1007/s11263-019-01228-7},
volume = {128},
year = {2016}
}

@inproceedings{Mahendran2016,
abstract = {Deconvolution is a popular method for visualizing deep convolutional neural networks; however, due to their heuristic nature, the meaning of deconvolutional visualizations is not entirely clear. In this paper, we introduce a family of reversed networks that generalizes and relates deconvolution, backpropagation and network saliency. We use this construction to thoroughly investigate and compare these methods in terms of quality and meaning of the produced images, and of what architectural choices are important in determining these properties. We also show an application of these generalized deconvolutional networks to weakly-supervised foreground object segmentation.},
author = {Mahendran, Aravindh and Vedaldi, Andrea},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46466-4_8},
isbn = {9783319464657},
issn = {16113349},
keywords = {CNN,DeConvNets,Deep convolutional neural networks,Saliency,Segmentation},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {CNN},
pages = {120--135},
publisher = {Springer, Cham},
title = {{Salient deconvolutional networks}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-46466-4_8},
volume = {9910 LNCS},
year = {2016}
}

@inproceedings{Ancona2018,
abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
archivePrefix = {arXiv},
arxivId = {1711.06104},
author = {Ancona, Marco and Ceolini, Enea and {\"{O}}ztireli, Cengiz and Gross, Markus},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
doi = {10.3929/ETHZ-B-000249929},
eprint = {1711.06104},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ancona et al. - Unknown - TOWARDS BETTER UNDERSTANDING OF GRADIENT-BASED ATTRIBUTION METHODS FOR DEEP NEURAL NETWORKS.pdf:pdf},
keywords = {CNN},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {CNN},
title = {{Towards better understanding of gradient-based attribution methods for deep neural networks}},
year = {2018}
}

@inproceedings{Shrikumar2017,
abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLlFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLlFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLlFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLlFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
archivePrefix = {arXiv},
arxivId = {1704.02685},
author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1704.02685},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrikumar, Greenside, Kundaje - 2017 - Learning Important Features Through Propagating Activation Differences.pdf:pdf},
isbn = {9781510855144},
keywords = {CNN},
mendeley-groups = {Feature Effect paper},
mendeley-tags = {CNN},
month = {apr},
pages = {4844--4866},
publisher = {International Machine Learning Society (IMLS)},
title = {{Learning important features through propagating activation differences}},
url = {https://arxiv.org/abs/1704.02685v2},
volume = {7},
year = {2017}
}

@article{bau2020units,
  author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
  title = {Understanding the role of individual units in a deep neural network},
  elocation-id = {201907375},
  year = {2020},
  doi = {10.1073/pnas.1907375117},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  URL = {https://www.pnas.org/content/early/2020/08/31/1907375117},
  journal = {Proceedings of the National Academy of Sciences}
}

@article{Gurumoorthy2019,
archivePrefix = {arXiv},
arxivId = {1707.01212},
author = {Gurumoorthy, Karthik S. and Dhurandhar, Amit and Cecchi, Guillermo and Aggarwal, Charu},
doi = {10.1109/ICDM.2019.00036},
eprint = {1707.01212},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gurumoorthy et al. - 2017 - Efficient Data Representation by Selecting Prototypes with Importance Weights.pdf:pdf},
isbn = {9781728146034},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Data summarization,Outlier detection,Prototype selection,Submodularity},
mendeley-groups = {org-feature-effect},
month = {jul},
pages = {260--269},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Efficient data representation by selecting prototypes with importance weights}},
url = {https://arxiv.org/abs/1707.01212v4},
volume = {2019-November},
year = {2019}
}

@article{Guidotti2018,
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
archivePrefix = {arXiv},
arxivId = {1802.01933},
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
doi = {10.1145/3236009},
eprint = {1802.01933},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guidotti et al. - 2018 - A survey of methods for explaining black box models.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Explanations,Interpretability,Open The Black Box,Transparent Models},
mendeley-groups = {org-feature-effect},
month = {feb},
number = {5},
publisher = {Association for Computing Machinery},
title = {{A survey of methods for explaining black box models}},
url = {https://arxiv.org/abs/1802.01933v3},
volume = {51},
year = {2018}
}

@article{Friedman2008,
abstract = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principal advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as is its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the identities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects. {\textcopyright} Institute of Mathematical Statistics.},
archivePrefix = {arXiv},
arxivId = {0811.1679v1},
author = {Friedman, Jerome H. and Popescu, Bogdan E.},
doi = {10.1214/07-AOAS148},
eprint = {0811.1679v1},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman, Popescu - 2008 - Predictive learning via rule ensembles.pdf:pdf},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Classification,Data mining,Interaction effects,Learning ensembles,Machine learning,Regression,Rules,Variable importance},
mendeley-groups = {org-feature-effect},
month = {nov},
number = {3},
pages = {916--954},
title = {{Predictive learning via rule ensembles}},
url = {http://arxiv.org/abs/0811.1679 http://dx.doi.org/10.1214/07-AOAS148},
volume = {2},
year = {2008}
}

@article{Inglis2022,
abstract = {Variable importance, interaction measures, and partial dependence plots are important summaries in the interpretation of statistical and machine learning models. In this article, we describe new visualization techniques for exploring these model summaries. We construct heatmap and graph-based displays showing variable importance and interaction jointly, which are carefully designed to highlight important aspects of the fit. We describe a new matrix-type layout showing all single and bivariate partial dependence plots, and an alternative layout based on graph Eulerians focusing on key subsets. Our new visualizations are model-agnostic and are applicable to regression and classification supervised learning settings. They enhance interpretation even in situations where the number of variables is large. Our R package vivid (variable importance and variable interaction displays) provides an implementation. Supplementary files for this article are available online.},
archivePrefix = {arXiv},
arxivId = {2108.04310},
author = {Inglis, Alan and Parnell, Andrew and Hurley, Catherine B.},
doi = {10.1080/10618600.2021.2007935},
eprint = {2108.04310},
file = {:home/givasile/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Inglis, Parnell, Hurley - 2021 - Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models.pdf:pdf},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Black-box,Model explanation,Model visualization},
mendeley-groups = {org-feature-effect},
month = {aug},
publisher = {American Statistical Association},
title = {{Visualizing Variable Importance and Variable Interaction Effects in Machine Learning Models}},
url = {https://arxiv.org/abs/2108.04310v2},
year = {2022}
}

@misc{Baniecki2022,
  doi = {10.48550/ARXIV.2105.12837},
  url = {https://arxiv.org/abs/2105.12837},
  author = {Baniecki, Hubert and Kretowicz, Wojciech and Biecek, Przemyslaw},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Fooling Partial Dependence via Data Poisoning},
  publisher = {arXiv},
  year = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}